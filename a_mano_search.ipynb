{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***SIMULACIONES***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulación Baja Resolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gurobipy import Model, GRB, quicksum\n",
    "import networkx as nx\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import random\n",
    "import gurobipy as gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "#Compatibility\n",
    "compatibility = pd.read_csv('/Users/valentina/Library/CloudStorage/OneDrive - UAI/TESIS/CODIGO/Datas_simulaciones/compatibilidad_total.csv', index_col=0)\n",
    "compatibility.index = range(len(compatibility))\n",
    "compatibility.columns = range(len(compatibility.columns))\n",
    "#Pairs, recipients and donors\n",
    "pairs = pd.read_csv('/Users/valentina/Library/CloudStorage/OneDrive - UAI/TESIS/CODIGO/Datas_simulaciones/parejas.csv',index_col=0)\n",
    "recipients = pd.read_csv('/Users/valentina/Library/CloudStorage/OneDrive - UAI/TESIS/CODIGO/Datas_simulaciones/receptores.csv',index_col=0)\n",
    "donors = pd.read_csv('/Users/valentina/Library/CloudStorage/OneDrive - UAI/TESIS/CODIGO/Datas_simulaciones/donantes.csv',index_col=0)\n",
    "#Weights\n",
    "hla_hr = pd.read_csv(\"/Users/valentina/Library/CloudStorage/OneDrive - UAI/TESIS/CODIGO/Datas_simulaciones/peso_ar.csv\",index_col=0)\n",
    "hla_lr = pd.read_csv(\"/Users/valentina/Library/CloudStorage/OneDrive - UAI/TESIS/CODIGO/Datas_simulaciones/peso_BR.csv\",index_col=0)\n",
    "hla_eplet = pd.read_csv(\"/Users/valentina/Library/CloudStorage/OneDrive - UAI/TESIS/CODIGO/Datas_simulaciones/peso_eplet.csv\",index_col=0)\n",
    "hla_hr.columns = hla_hr.columns.astype(int)\n",
    "hla_lr.columns = hla_lr.columns.astype(int)\n",
    "hla_eplet.columns = hla_eplet.columns.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of each locus \n",
    "base = '/Users/valentina/Library/CloudStorage/OneDrive - UAI/TESIS/CODIGO/Datas_simulaciones/locis_por_separado/BR/'\n",
    "mats = {\n",
    "    'A': 'mismatch_BR_A.csv',\n",
    "    'B': 'mismatch_BR_B.csv',\n",
    "    'C': 'mismatch_BR_C.csv',\n",
    "    'DQ': 'mismatch_BR_DQ.csv',\n",
    "    'DR': 'mismatch_BR_DR.csv'\n",
    "}\n",
    "\n",
    "matrix_mismatch = {}\n",
    "for locus, archive in mats.items():\n",
    "    matrix_mismatch[locus] = pd.read_csv(base + archive, index_col=0)\n",
    "\n",
    "mismatch_ClassI = (\n",
    "    matrix_mismatch['A'] +\n",
    "    matrix_mismatch['B'] +\n",
    "    matrix_mismatch['C']\n",
    ")\n",
    "\n",
    "mismatch_DQ = matrix_mismatch['DQ']\n",
    "mismatch_DR = matrix_mismatch['DR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mismatch to HLA score\n",
    "HLA_DR = 2 - mismatch_DR\n",
    "HLA_DQ = 2- mismatch_DQ\n",
    "HLA_C1 = 6 - mismatch_ClassI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of initial graph with graph resolution and a minimum of quality (k)\n",
    "def create_graph(pairs, compatibility, hla1, k):\n",
    "    G = nx.DiGraph()\n",
    "    added_edges = 0\n",
    "    for i in pairs.index:\n",
    "        for j in pairs.index:\n",
    "            if compatibility.at[i, j] == 1 and hla1.at[i, j] >= k:\n",
    "                G.add_edge(j, i, weight=hla1.at[i, j])\n",
    "                added_edges += 1\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing initial weights to optimization weights on the initial graph arcs\n",
    "def changing_resolution_weights(G, hla2):\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        try:\n",
    "            data['weight'] = hla2.iloc[int(v), int(u)]\n",
    "        except KeyError:\n",
    "            print(f\"No se encontró peso para el arco ({u}, {v})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al actualizar peso para el arco ({u}, {v}): {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR OPTUNA\n",
    "\n",
    "def optimization(G, multipliers_ethnicity, l=3, k=3):\n",
    "    total_cycles = list(nx.simple_cycles(G, length_bound=3))\n",
    "    valid_cycles = [cycle for cycle in total_cycles if len(cycle) <= l and all(G[u][v]['weight'] >= k for u, v in zip(cycle, cycle[1:] + cycle[:1]))]\n",
    "    \n",
    "    P = len(G.nodes())  \n",
    "    Z = 10\n",
    "\n",
    "\n",
    "\n",
    "    # # ---- imprimir desglose ----\n",
    "    # for cycle in valid_cycles[:5]:  \n",
    "    #     print(f\"\\nCiclo {cycle}:\")\n",
    "    #     term = 0\n",
    "    #     for u, v in zip(cycle, cycle[1:] + cycle[:1]):\n",
    "    #         eth = recipients.loc[v, 'ETHCAT']\n",
    "    #         mult = multipliers_ethnicity.get(eth, 1.0)\n",
    "    #         w = G[u][v]['weight']\n",
    "\n",
    "    #         part1 = mult * 1\n",
    "    #         part2 = mult * (w / (P * Z))\n",
    "\n",
    "    #         term = term + part1 + part2\n",
    "\n",
    "\n",
    "    #         print(f\"  {u}->{v} | ETHCAT={eth} | multiplier={mult}\")\n",
    "    #         print(f\"    multiplier * 1 = {part1:.6f}\")\n",
    "    #         print(f\"    + multiplier * (w/(P*Z)) = {part2:.6f}  (w={w}, P={P}, Z={Z})\")\n",
    "\n",
    "    #     print(\"Term:\", term )\n",
    "    #     print(\"-\" * 50)\n",
    "\n",
    "\n",
    "    m = Model(\"optimization\")\n",
    "    m.setParam('OutputFlag', 0)\n",
    "\n",
    "    x = {\n",
    "        tuple(cycle): m.addVar(vtype=GRB.BINARY, \n",
    "                               name=f\"x_{'_'.join(map(str, cycle))}\") \n",
    "        for cycle in valid_cycles\n",
    "    }\n",
    "\n",
    "    m.setObjective(\n",
    "        quicksum(\n",
    "            x[tuple(cycle)] * \n",
    "            #(\n",
    "                sum(\n",
    "                multipliers_ethnicity.get(recipients.loc[v, 'ETHCAT'], 1.0) *\n",
    "                (1 + (G[u][v]['weight'] / (P * Z)))\n",
    "                for u, v in zip(cycle, cycle[1:] + cycle[:1])\n",
    "            )\n",
    "            #)/P\n",
    "            \n",
    "            for cycle in valid_cycles\n",
    "        ),\n",
    "        GRB.MAXIMIZE\n",
    "    )\n",
    "\n",
    "    for i in G.nodes():\n",
    "        m.addConstr(quicksum(x[tuple(cycle)] for cycle in valid_cycles if i in cycle) <= 1, \n",
    "                    name=f\"node_usage_{i}\")\n",
    "\n",
    "    try:\n",
    "        m.optimize()\n",
    "    except gp.GurobiError as e:\n",
    "        print(f\"⚠️ Error of Gurobi: {e}\")\n",
    "      \n",
    "        return None, [], True  \n",
    "\n",
    "    G_optimal = nx.DiGraph()\n",
    "    selected_cycles = []\n",
    "\n",
    "    if m.status == GRB.OPTIMAL:\n",
    "        for cycle in valid_cycles:\n",
    "            if x[tuple(cycle)].X > 0.5:\n",
    "                selected_cycles.append(cycle)\n",
    "                for i in range(len(cycle)):\n",
    "                    u, v = cycle[i], cycle[(i + 1) % len(cycle)]\n",
    "                    G_optimal.add_edge(u, v, weight=G[u][v]['weight'])\n",
    "\n",
    "\n",
    "    return G_optimal, selected_cycles, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete definition of the simulation\n",
    "\n",
    "def update_matrices(indexes, df):\n",
    "    return df.iloc[indexes, indexes]\n",
    "frequiency_waiting_nodes = {}\n",
    "\n",
    "def run_simulation(total_time, arrival_rate, departure_rate, match_run, pairs, compatibility, hla_lr, hla_hr, hla_eplet, multipliers_ethnicity, seed= None):\n",
    "    # Evaluation at different resolutions\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    lr_quality_ethcat1 = []\n",
    "    lr_quality_ethcat2 = []\n",
    "    lr_quality_ethcat4 = []\n",
    "    lr_quality_ethcat5 = []\n",
    "    lr_quality_ethcat6 = []\n",
    "    lr_quality_ethcat7 = []\n",
    "    \n",
    "    hr_quality_ethcat1 = []\n",
    "    hr_quality_ethcat2 = []\n",
    "    hr_quality_ethcat4 = []\n",
    "    hr_quality_ethcat5 = []\n",
    "    hr_quality_ethcat6 = []\n",
    "    hr_quality_ethcat7 = []\n",
    "\n",
    "    e_quality_ethcat1 = []\n",
    "    e_quality_ethcat2 = []\n",
    "    e_quality_ethcat4 = []\n",
    "    e_quality_ethcat5 = []\n",
    "    e_quality_ethcat6 = []\n",
    "    e_quality_ethcat7 = []\n",
    "\n",
    "    waiting_list = []\n",
    "    waiting_times = {}  \n",
    "    arrivals_by_ethcat = {}\n",
    "    historial_cycles = []\n",
    "    historial_departures = []\n",
    "    cont=0\n",
    "    available_indexes = set(pairs.index)\n",
    "\n",
    "    \n",
    "    departures_by_ethcat = {1: [], 2: [], 4: [], 5: [], 6: [], 7: []}\n",
    "    HLA_classI_ethcat = {1: [], 2: [], 4: [], 5: [], 6: [], 7: []}\n",
    "    HLA_DR_ethcat = {1: [], 2: [], 4: [], 5: [], 6: [], 7: []}\n",
    "    HLA_DQ_ethcat = {1: [], 2: [], 4: [], 5: [], 6: [], 7: []}\n",
    "\n",
    "    # Simulation per month\n",
    "    for month in range(total_time):\n",
    "     \n",
    "        selected_cycles = []\n",
    "       \n",
    "        new_entries = np.random.poisson(arrival_rate)\n",
    "        new_entries = min(new_entries, len(available_indexes))\n",
    "        new_index = np.random.choice(list(available_indexes), size=new_entries, replace=False)\n",
    "        cont+= len(new_index)\n",
    "        waiting_list.extend(new_index)\n",
    "        available_indexes.difference_update(new_index)\n",
    "\n",
    "        for idx in new_index:\n",
    "            waiting_times[idx] = {'arrival': month}\n",
    "            ethnicity = recipients.loc[recipients['Nodo'] == idx, 'ETHCAT'].iloc[0]\n",
    "            if ethnicity in arrivals_by_ethcat:\n",
    "                arrivals_by_ethcat[ethnicity] += 1\n",
    "            else:\n",
    "                arrivals_by_ethcat[ethnicity] = 1\n",
    "        \n",
    " \n",
    "        departure = np.random.poisson(departure_rate)\n",
    "        departure_indexes = [] \n",
    "        if departure:\n",
    "            departure = min(departure, len(waiting_list))\n",
    "            departure_indexes = np.random.choice(waiting_list, size=departure, replace=False)\n",
    "            waiting_list = [idx for idx in waiting_list if idx not in departure_indexes]\n",
    "            historial_departures.extend(departure_indexes)\n",
    "\n",
    "        for idx in departure_indexes:\n",
    "            ethnicity = recipients.loc[idx, 'ETHCAT']\n",
    "            departures_by_ethcat[ethnicity].append(idx)\n",
    "\n",
    "        if (month + 1) % match_run == 0:\n",
    "            waiting_list_index = waiting_list.copy()\n",
    "            df_waiting_list = pairs.loc[waiting_list_index]\n",
    "            \n",
    "            filtered_compatibility = update_matrices(waiting_list_index, compatibility)\n",
    "            filtered_weight = update_matrices(waiting_list_index, hla_lr) # Insert the resolution data that you want for create the graph\n",
    "            \n",
    "    \n",
    "            G = create_graph(df_waiting_list, filtered_compatibility, filtered_weight, k= 3) # Insert the minimum weight for create the graph \n",
    "            changing_resolution_weights(G, hla_lr) # Insert the resolution data that you want for the optimization\n",
    "\n",
    "            \n",
    "            \n",
    "            G_optimal, selected_cycles,  error_gurobi = optimization(G, multipliers_ethnicity, l=3, k=3 ) # Insert the minimum quality that you want for the cycles \n",
    "\n",
    "            if error_gurobi:\n",
    "                return float(\"inf\"), None\n",
    "\n",
    "            for u, v, data in G_optimal.edges(data=True):\n",
    "                node_ethcat = recipients.loc[v, 'ETHCAT'] \n",
    "                \n",
    "                # HLA for differents loci (optimization resolution)\n",
    "                #value_HLA_classI = HLA_C1.iloc[v, u]\n",
    "                value_HLA_DR = HLA_DR.iloc[v, u]\n",
    "                value_HLA_DQ = HLA_DQ.iloc[v, u]\n",
    "\n",
    "                #HLA_classI_ethcat[node_ethcat].append(value_HLA_classI)\n",
    "                HLA_DR_ethcat[node_ethcat].append(value_HLA_DR)\n",
    "                HLA_DQ_ethcat[node_ethcat].append(value_HLA_DQ)\n",
    "\n",
    "\n",
    "                if node_ethcat == 1:\n",
    "                    lr_quality_ethcat1.append(hla_lr.iloc[v, u])\n",
    "                    hr_quality_ethcat1.append(hla_hr.iloc[v, u])\n",
    "                    e_quality_ethcat1.append(hla_eplet.iloc[v, u])\n",
    "                elif node_ethcat == 2:\n",
    "                    lr_quality_ethcat2.append(hla_lr.iloc[v, u])\n",
    "                    hr_quality_ethcat2.append(hla_hr.iloc[v, u])\n",
    "                    e_quality_ethcat2.append(hla_eplet.iloc[v, u])\n",
    "                elif node_ethcat == 4:\n",
    "                    lr_quality_ethcat4.append(hla_lr.iloc[v, u])\n",
    "                    hr_quality_ethcat4.append(hla_hr.iloc[v, u])\n",
    "                    e_quality_ethcat4.append(hla_eplet.iloc[v, u])\n",
    "                elif node_ethcat == 5:\n",
    "                    lr_quality_ethcat5.append(hla_lr.iloc[v, u])\n",
    "                    hr_quality_ethcat5.append(hla_hr.iloc[v, u])\n",
    "                    e_quality_ethcat5.append(hla_eplet.iloc[v, u])\n",
    "                elif node_ethcat == 6:\n",
    "                    lr_quality_ethcat6.append(hla_lr.iloc[v, u])\n",
    "                    hr_quality_ethcat6.append(hla_hr.iloc[v, u])\n",
    "                    e_quality_ethcat6.append(hla_eplet.iloc[v, u])\n",
    "                elif node_ethcat == 7:\n",
    "                    lr_quality_ethcat7.append(hla_lr.iloc[v, u])\n",
    "                    hr_quality_ethcat7.append(hla_hr.iloc[v, u])\n",
    "                    e_quality_ethcat7.append(hla_eplet.iloc[v, u])\n",
    "            historial_cycles.extend(selected_cycles)\n",
    "\n",
    "            nodes_in_cycles = [node for cycle in selected_cycles for node in cycle]\n",
    "            waiting_list = [idx for idx in waiting_list if idx not in nodes_in_cycles]\n",
    "            for idx in nodes_in_cycles:\n",
    "                waiting_times[idx]['departure'] = month\n",
    "\n",
    "    nodes_in_historial = [node for cycle in historial_cycles for node in cycle]\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for idx in nodes_in_historial:\n",
    "        if idx in waiting_times:\n",
    "            waiting_time = waiting_times[idx]['departure'] - waiting_times[idx]['arrival']\n",
    "            ethnicity = recipients.loc[idx, 'ETHCAT'] \n",
    "            data.append({'node': idx, 'waiting time (months)': waiting_time, 'ethnicity': ethnicity})\n",
    "\n",
    "    \n",
    "\n",
    "    df_ethnicity = pd.DataFrame(data)\n",
    "    df_ethnicity['ethnicity'] = df_ethnicity['ethnicity'].astype(int)\n",
    "    df_ethnicity1 = df_ethnicity[df_ethnicity['ethnicity'] == 1]\n",
    "    df_ethnicity2 = df_ethnicity[df_ethnicity['ethnicity'] == 2]\n",
    "    df_ethnicity4 = df_ethnicity[df_ethnicity['ethnicity'] == 4]\n",
    "    df_ethnicity5 = df_ethnicity[df_ethnicity['ethnicity'] == 5]\n",
    "    df_ethnicity6 = df_ethnicity[df_ethnicity['ethnicity'] == 6]\n",
    "    df_ethnicity7 = df_ethnicity[df_ethnicity['ethnicity'] == 7]\n",
    "    waiting_time_mean_1 = df_ethnicity1['waiting time (months)'].mean()\n",
    "    waiting_time_mean_2 = df_ethnicity2['waiting time (months)'].mean()\n",
    "    waiting_time_mean_4 = df_ethnicity4['waiting time (months)'].mean()\n",
    "    waiting_time_mean_5 = df_ethnicity5['waiting time (months)'].mean()\n",
    "    waiting_time_mean_6 = df_ethnicity6['waiting time (months)'].mean()\n",
    "    waiting_time_mean_7 = df_ethnicity7['waiting time (months)'].mean()\n",
    "\n",
    "    \n",
    "    outgoing_per_ethcat = {e: len(departures_by_ethcat[e]) for e in [1,2,4,5,6,7]}\n",
    "    L_per_ethcat  = {e: outgoing_per_ethcat [e] / arrivals_by_ethcat.get(e, 1) for e in outgoing_per_ethcat } \n",
    "    F_per_ethcat  = {e: len([c for c in historial_cycles for n in c if recipients.loc[n,'ETHCAT']==e]) / arrivals_by_ethcat.get(e, 1)\n",
    "                   for e in [1,2,4,5,6,7]}  \n",
    "    \n",
    "     \n",
    "    total_trasplants = sum(len(cycle) for cycle in historial_cycles)\n",
    "    total_entries = sum(arrivals_by_ethcat.values())\n",
    "    F_total = total_trasplants / total_entries if total_entries > 0 else 0\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "    return {\n",
    "        'historial_cycles': historial_cycles,\n",
    "        'historial_departures': historial_departures,\n",
    "        'entries_nodes': cont,\n",
    "        'arrivals_by_ethcat': arrivals_by_ethcat,\n",
    "        'waiting_time_mean1':   waiting_time_mean_1 ,\n",
    "        'waiting_time_mean2':   waiting_time_mean_2,\n",
    "        'waiting_time_mean4':   waiting_time_mean_4,\n",
    "        'waiting_time_mean5':   waiting_time_mean_5,\n",
    "        'waiting_time_mean6':   waiting_time_mean_6,\n",
    "        'waiting_time_mean7':   waiting_time_mean_7,\n",
    "        'lr_quality_ethcat1': np.mean(lr_quality_ethcat1),\n",
    "        'lr_quality_ethcat2': np.mean(lr_quality_ethcat2),\n",
    "        'lr_quality_ethcat4': np.mean(lr_quality_ethcat4),\n",
    "        'lr_quality_ethcat5': np.mean(lr_quality_ethcat5),\n",
    "        'lr_quality_ethcat6': np.mean(lr_quality_ethcat6),\n",
    "        'lr_quality_ethcat7': np.mean(lr_quality_ethcat7),\n",
    "\n",
    "        'hr_quality_ethcat1': np.mean(hr_quality_ethcat1),\n",
    "        'hr_quality_ethcat2': np.mean(hr_quality_ethcat2),\n",
    "        'hr_quality_ethcat4': np.mean(hr_quality_ethcat4),\n",
    "        'hr_quality_ethcat5': np.mean(hr_quality_ethcat5),\n",
    "        'hr_quality_ethcat6': np.mean(hr_quality_ethcat6),\n",
    "        'hr_quality_ethcat7': np.mean(hr_quality_ethcat7),\n",
    "\n",
    "        'e_quality_ethcat1': np.mean(e_quality_ethcat1),\n",
    "        'e_quality_ethcat2': np.mean(e_quality_ethcat2),\n",
    "        'e_quality_ethcat4': np.mean(e_quality_ethcat4),\n",
    "        'e_quality_ethcat5': np.mean(e_quality_ethcat5),\n",
    "        'e_quality_ethcat6': np.mean(e_quality_ethcat6),\n",
    "        'e_quality_ethcat7': np.mean(e_quality_ethcat7),\n",
    "        'avg_ClassI_HLA_ethcat1': np.mean(HLA_classI_ethcat[1]), \n",
    "        'avg_ClassI_HLA_ethcat2': np.mean(HLA_classI_ethcat[2]),\n",
    "        'avg_ClassI_HLA_ethcat4': np.mean(HLA_classI_ethcat[4]),\n",
    "        'avg_ClassI_HLA_ethcat5': np.mean(HLA_classI_ethcat[5]),\n",
    "        'avg_ClassI_HLA_ethcat6': np.mean(HLA_classI_ethcat[6]),\n",
    "        'avg_ClassI_HLA_ethcat7': np.mean(HLA_classI_ethcat[7]), \n",
    "        'HLA_ClassI_total': np.mean(\n",
    "                                    HLA_classI_ethcat[1] + \n",
    "                                    HLA_classI_ethcat[2] + \n",
    "                                    HLA_classI_ethcat[4] + \n",
    "                                    HLA_classI_ethcat[5] + \n",
    "                                    HLA_classI_ethcat[6] + \n",
    "                                    HLA_classI_ethcat[7]\n",
    "                                ),\n",
    "\n",
    "        'avg_DR_HLA_ethcat1': np.mean(HLA_DR_ethcat[1]), \n",
    "        'avg_DR_HLA_ethcat2': np.mean(HLA_DR_ethcat[2]),\n",
    "        'avg_DR_HLA_ethcat4': np.mean(HLA_DR_ethcat[4]),\n",
    "        'avg_DR_HLA_ethcat5': np.mean(HLA_DR_ethcat[5]),\n",
    "        'avg_DR_HLA_ethcat6': np.mean(HLA_DR_ethcat[6]),\n",
    "        'avg_DR_HLA_ethcat7': np.mean(HLA_DR_ethcat[7]), \n",
    "        'HLA_DR_total': np.mean(\n",
    "                                    HLA_DR_ethcat[1] + \n",
    "                                    HLA_DR_ethcat[2] + \n",
    "                                    HLA_DR_ethcat[4] + \n",
    "                                    HLA_DR_ethcat[5] + \n",
    "                                    HLA_DR_ethcat[6] + \n",
    "                                    HLA_DR_ethcat[7]\n",
    "                                ),\n",
    "\n",
    "        'avg_DQ_HLA_ethcat1': np.mean(HLA_DQ_ethcat[1]), \n",
    "        'avg_DQ_HLA_ethcat2': np.mean(HLA_DQ_ethcat[2]),\n",
    "        'avg_DQ_HLA_ethcat4': np.mean(HLA_DQ_ethcat[4]),\n",
    "        'avg_DQ_HLA_ethcat5': np.mean(HLA_DQ_ethcat[5]),\n",
    "        'avg_DQ_HLA_ethcat6': np.mean(HLA_DQ_ethcat[6]),\n",
    "        'avg_DQ_HLA_ethcat7': np.mean(HLA_DQ_ethcat[7]), \n",
    "        'HLA_DQ_total': np.mean(\n",
    "                                    HLA_DQ_ethcat[1] + \n",
    "                                    HLA_DQ_ethcat[2] + \n",
    "                                    HLA_DQ_ethcat[4] + \n",
    "                                    HLA_DQ_ethcat[5] + \n",
    "                                    HLA_DQ_ethcat[6] + \n",
    "                                    HLA_DQ_ethcat[7]\n",
    "                                ),\n",
    "\n",
    "        'F_per_ethcat': F_per_ethcat,        \n",
    "        'L_per_ethcat' : L_per_ethcat,          \n",
    "        'F_total': F_total,\n",
    "        'lr_quality_total': np.mean(\n",
    "                                    lr_quality_ethcat1 + \n",
    "                                    lr_quality_ethcat2 + \n",
    "                                    lr_quality_ethcat4 + \n",
    "                                    lr_quality_ethcat5 + \n",
    "                                    lr_quality_ethcat6 + \n",
    "                                    lr_quality_ethcat7\n",
    "                                ),\n",
    "        'hr_quality_total': np.mean(\n",
    "                                    hr_quality_ethcat1 + \n",
    "                                    hr_quality_ethcat2 + \n",
    "                                    hr_quality_ethcat4 + \n",
    "                                    hr_quality_ethcat5 + \n",
    "                                    hr_quality_ethcat6 + \n",
    "                                    hr_quality_ethcat7\n",
    "                                ),\n",
    "        'e_quality_total': np.mean(\n",
    "                                    e_quality_ethcat1 + \n",
    "                                    e_quality_ethcat2 + \n",
    "                                    e_quality_ethcat4 + \n",
    "                                    e_quality_ethcat5 + \n",
    "                                    e_quality_ethcat6 + \n",
    "                                    e_quality_ethcat7\n",
    "                                ),\n",
    "        'Waiting_time_mean': pd.concat([\n",
    "                                    df_ethnicity1['waiting time (months)'],\n",
    "                                    df_ethnicity2['waiting time (months)'],\n",
    "                                    df_ethnicity4['waiting time (months)'],\n",
    "                                    df_ethnicity5['waiting time (months)'],\n",
    "                                    df_ethnicity6['waiting time (months)'],\n",
    "                                    df_ethnicity7['waiting time (months)']\n",
    "                                    ]).mean()\n",
    "\n",
    "\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### For the same seed just in the iterations ######\n",
    "def run_simulation(\n",
    "    total_time,\n",
    "    arrival_rate,\n",
    "    departure_rate,\n",
    "    match_run,\n",
    "    pairs,\n",
    "    compatibility,\n",
    "    hla_lr,\n",
    "    hla_hr,\n",
    "    hla_eplet,\n",
    "    multipliers_ethnicity,\n",
    "    seed=None\n",
    "):\n",
    "    if seed is None:\n",
    "        rng_arr = np.random.RandomState()   # arrivals\n",
    "        rng_dep = np.random.RandomState()   # departures\n",
    "    else:\n",
    "        rng_arr = np.random.RandomState(seed + 1) \n",
    "        rng_dep = np.random.RandomState(seed + 2)  \n",
    "\n",
    "    # accumulators\n",
    "    lr_quality_ethcat1, lr_quality_ethcat2, lr_quality_ethcat4 = [], [], []\n",
    "    lr_quality_ethcat5, lr_quality_ethcat6, lr_quality_ethcat7 = [], [], []\n",
    "    hr_quality_ethcat1, hr_quality_ethcat2, hr_quality_ethcat4 = [], [], []\n",
    "    hr_quality_ethcat5, hr_quality_ethcat6, hr_quality_ethcat7 = [], [], []\n",
    "    e_quality_ethcat1,  e_quality_ethcat2,  e_quality_ethcat4  = [], [], []\n",
    "    e_quality_ethcat5,  e_quality_ethcat6,  e_quality_ethcat7  = [], [], []\n",
    "\n",
    "    waiting_list = []\n",
    "    waiting_times = {}\n",
    "    arrivals_by_ethcat = {}\n",
    "    historial_cycles = []\n",
    "    historial_departures = []\n",
    "    cont = 0\n",
    "\n",
    "    departures_by_ethcat = {1: [], 2: [], 4: [], 5: [], 6: [], 7: []}\n",
    "    HLA_classI_ethcat = {1: [], 2: [], 4: [], 5: [], 6: [], 7: []}\n",
    "    HLA_DR_ethcat     = {1: [], 2: [], 4: [], 5: [], 6: [], 7: []}\n",
    "    HLA_DQ_ethcat     = {1: [], 2: [], 4: [], 5: [], 6: [], 7: []}\n",
    "\n",
    "    pool = np.array(pairs.index, copy=True)\n",
    "    pool = rng_arr.permutation(pool)\n",
    "    ptr = 0  \n",
    "\n",
    "    # simulation month per month\n",
    "    for month in range(total_time):\n",
    "        selected_cycles = []\n",
    "\n",
    "        # arrivals\n",
    "        new_entries = int(rng_arr.poisson(arrival_rate))\n",
    "        remaining = len(pool) - ptr\n",
    "        if remaining <= 0:\n",
    "            new_entries = 0\n",
    "        else:\n",
    "            new_entries = min(new_entries, remaining)\n",
    "\n",
    "        if new_entries > 0:\n",
    "            new_index = pool[ptr:ptr + new_entries]\n",
    "            ptr += new_entries\n",
    "            cont += new_entries\n",
    "            waiting_list.extend(new_index.tolist())\n",
    "\n",
    "            for idx in new_index:\n",
    "                waiting_times[idx] = {'arrival': month}\n",
    "                ethnicity = recipients.loc[recipients['Nodo'] == idx, 'ETHCAT'].iloc[0]\n",
    "                arrivals_by_ethcat[ethnicity] = arrivals_by_ethcat.get(ethnicity, 0) + 1\n",
    "\n",
    "        # departures\n",
    "        departure = int(rng_dep.poisson(departure_rate))\n",
    "        if departure > 0 and len(waiting_list) > 0:\n",
    "            departure = min(departure, len(waiting_list))\n",
    "            pos = rng_dep.choice(len(waiting_list), size=departure, replace=False)\n",
    "            pos.sort()\n",
    "            departure_indexes = [waiting_list[p] for p in pos]\n",
    "            keep = set(departure_indexes)\n",
    "            waiting_list = [idx for idx in waiting_list if idx not in keep]\n",
    "            historial_departures.extend(departure_indexes)\n",
    "\n",
    "            for idx in departure_indexes:\n",
    "                ethnicity = recipients.loc[idx, 'ETHCAT']\n",
    "                departures_by_ethcat[ethnicity].append(idx)\n",
    "\n",
    "        # match run\n",
    "        if (month + 1) % match_run == 0 and len(waiting_list) > 0:\n",
    "            waiting_list_index = waiting_list.copy()\n",
    "            df_waiting_list = pairs.loc[waiting_list_index]\n",
    "\n",
    "            filtered_compatibility = update_matrices(waiting_list_index, compatibility)\n",
    "            filtered_weight = update_matrices(waiting_list_index, hla_eplet)\n",
    "\n",
    "            G = create_graph(df_waiting_list, filtered_compatibility, filtered_weight, k=45)\n",
    "            changing_resolution_weights(G, hla_eplet)\n",
    "\n",
    "            G_optimal, selected_cycles, error_gurobi = optimization(G, multipliers_ethnicity, l=3, k=45)\n",
    "            if error_gurobi:\n",
    "                return float(\"inf\"), None\n",
    "\n",
    "            for u, v, data in G_optimal.edges(data=True):\n",
    "                node_ethcat = recipients.loc[v, 'ETHCAT']\n",
    "\n",
    "                value_HLA_classI = HLA_C1.iloc[v, u]\n",
    "                value_HLA_DR     = HLA_DR.iloc[v, u]\n",
    "                value_HLA_DQ     = HLA_DQ.iloc[v, u]\n",
    "\n",
    "                HLA_classI_ethcat[node_ethcat].append(value_HLA_classI)\n",
    "                HLA_DR_ethcat[node_ethcat].append(value_HLA_DR)\n",
    "                HLA_DQ_ethcat[node_ethcat].append(value_HLA_DQ)\n",
    "\n",
    "                if node_ethcat == 1:\n",
    "                    lr_quality_ethcat1.append(hla_lr.iloc[v, u]); hr_quality_ethcat1.append(hla_hr.iloc[v, u]); e_quality_ethcat1.append(hla_eplet.iloc[v, u])\n",
    "                elif node_ethcat == 2:\n",
    "                    lr_quality_ethcat2.append(hla_lr.iloc[v, u]); hr_quality_ethcat2.append(hla_hr.iloc[v, u]); e_quality_ethcat2.append(hla_eplet.iloc[v, u])\n",
    "                elif node_ethcat == 4:\n",
    "                    lr_quality_ethcat4.append(hla_lr.iloc[v, u]); hr_quality_ethcat4.append(hla_hr.iloc[v, u]); e_quality_ethcat4.append(hla_eplet.iloc[v, u])\n",
    "                elif node_ethcat == 5:\n",
    "                    lr_quality_ethcat5.append(hla_lr.iloc[v, u]); hr_quality_ethcat5.append(hla_hr.iloc[v, u]); e_quality_ethcat5.append(hla_eplet.iloc[v, u])\n",
    "                elif node_ethcat == 6:\n",
    "                    lr_quality_ethcat6.append(hla_lr.iloc[v, u]); hr_quality_ethcat6.append(hla_hr.iloc[v, u]); e_quality_ethcat6.append(hla_eplet.iloc[v, u])\n",
    "                elif node_ethcat == 7:\n",
    "                    lr_quality_ethcat7.append(hla_lr.iloc[v, u]); hr_quality_ethcat7.append(hla_hr.iloc[v, u]); e_quality_ethcat7.append(hla_eplet.iloc[v, u])\n",
    "\n",
    "            historial_cycles.extend(selected_cycles)\n",
    "\n",
    "            nodes_in_cycles = [node for cycle in selected_cycles for node in cycle]\n",
    "            waiting_list = [idx for idx in waiting_list if idx not in nodes_in_cycles]\n",
    "            for idx in nodes_in_cycles:\n",
    "                waiting_times[idx]['departure'] = month\n",
    "\n",
    "    # some metrics\n",
    "    nodes_in_historial = [node for cycle in historial_cycles for node in cycle]\n",
    "    data = []\n",
    "    for idx in nodes_in_historial:\n",
    "        if idx in waiting_times:\n",
    "            waiting_time = waiting_times[idx]['departure'] - waiting_times[idx]['arrival']\n",
    "            ethnicity = recipients.loc[idx, 'ETHCAT']\n",
    "            data.append({'node': idx, 'waiting time (months)': waiting_time, 'ethnicity': ethnicity})\n",
    "\n",
    "    df_ethnicity = pd.DataFrame(data)\n",
    "    if not df_ethnicity.empty:\n",
    "        df_ethnicity['ethnicity'] = df_ethnicity['ethnicity'].astype(int)\n",
    "        df_ethnicity1 = df_ethnicity[df_ethnicity['ethnicity'] == 1]\n",
    "        df_ethnicity2 = df_ethnicity[df_ethnicity['ethnicity'] == 2]\n",
    "        df_ethnicity4 = df_ethnicity[df_ethnicity['ethnicity'] == 4]\n",
    "        df_ethnicity5 = df_ethnicity[df_ethnicity['ethnicity'] == 5]\n",
    "        df_ethnicity6 = df_ethnicity[df_ethnicity['ethnicity'] == 6]\n",
    "        df_ethnicity7 = df_ethnicity[df_ethnicity['ethnicity'] == 7]\n",
    "        waiting_time_mean_1 = df_ethnicity1['waiting time (months)'].mean()\n",
    "        waiting_time_mean_2 = df_ethnicity2['waiting time (months)'].mean()\n",
    "        waiting_time_mean_4 = df_ethnicity4['waiting time (months)'].mean()\n",
    "        waiting_time_mean_5 = df_ethnicity5['waiting time (months)'].mean()\n",
    "        waiting_time_mean_6 = df_ethnicity6['waiting time (months)'].mean()\n",
    "        waiting_time_mean_7 = df_ethnicity7['waiting time (months)'].mean()\n",
    "    else:\n",
    "        waiting_time_mean_1 = waiting_time_mean_2 = waiting_time_mean_4 = np.nan\n",
    "        waiting_time_mean_5 = waiting_time_mean_6 = waiting_time_mean_7 = np.nan\n",
    "\n",
    "    outgoing_per_ethcat = {e: len(departures_by_ethcat[e]) for e in [1,2,4,5,6,7]}\n",
    "    L_per_ethcat  = {e: outgoing_per_ethcat[e] / arrivals_by_ethcat.get(e, 1) for e in outgoing_per_ethcat}\n",
    "    F_per_ethcat  = {e: len([c for c in historial_cycles for n in c if recipients.loc[n, 'ETHCAT'] == e]) / arrivals_by_ethcat.get(e, 1)\n",
    "                     for e in [1,2,4,5,6,7]}\n",
    "\n",
    "    total_trasplants = sum(len(cycle) for cycle in historial_cycles)\n",
    "    total_entries = sum(arrivals_by_ethcat.values())\n",
    "    F_total = total_trasplants / total_entries if total_entries > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'historial_cycles': historial_cycles,\n",
    "        'historial_departures': historial_departures,\n",
    "        'entries_nodes': cont,\n",
    "        'arrivals_by_ethcat': arrivals_by_ethcat,\n",
    "        'waiting_time_mean1': waiting_time_mean_1,\n",
    "        'waiting_time_mean2': waiting_time_mean_2,\n",
    "        'waiting_time_mean4': waiting_time_mean_4,\n",
    "        'waiting_time_mean5': waiting_time_mean_5,\n",
    "        'waiting_time_mean6': waiting_time_mean_6,\n",
    "        'waiting_time_mean7': waiting_time_mean_7,\n",
    "\n",
    "        'lr_quality_ethcat1': np.mean(lr_quality_ethcat1),\n",
    "        'lr_quality_ethcat2': np.mean(lr_quality_ethcat2),\n",
    "        'lr_quality_ethcat4': np.mean(lr_quality_ethcat4),\n",
    "        'lr_quality_ethcat5': np.mean(lr_quality_ethcat5),\n",
    "        'lr_quality_ethcat6': np.mean(lr_quality_ethcat6),\n",
    "        'lr_quality_ethcat7': np.mean(lr_quality_ethcat7),\n",
    "\n",
    "        'hr_quality_ethcat1': np.mean(hr_quality_ethcat1),\n",
    "        'hr_quality_ethcat2': np.mean(hr_quality_ethcat2),\n",
    "        'hr_quality_ethcat4': np.mean(hr_quality_ethcat4),\n",
    "        'hr_quality_ethcat5': np.mean(hr_quality_ethcat5),\n",
    "        'hr_quality_ethcat6': np.mean(hr_quality_ethcat6),\n",
    "        'hr_quality_ethcat7': np.mean(hr_quality_ethcat7),\n",
    "\n",
    "        'e_quality_ethcat1': np.mean(e_quality_ethcat1),\n",
    "        'e_quality_ethcat2': np.mean(e_quality_ethcat2),\n",
    "        'e_quality_ethcat4': np.mean(e_quality_ethcat4),\n",
    "        'e_quality_ethcat5': np.mean(e_quality_ethcat5),\n",
    "        'e_quality_ethcat6': np.mean(e_quality_ethcat6),\n",
    "        'e_quality_ethcat7': np.mean(e_quality_ethcat7),\n",
    "\n",
    "        'avg_ClassI_HLA_ethcat1': np.mean(HLA_classI_ethcat[1]),\n",
    "        'avg_ClassI_HLA_ethcat2': np.mean(HLA_classI_ethcat[2]),\n",
    "        'avg_ClassI_HLA_ethcat4': np.mean(HLA_classI_ethcat[4]),\n",
    "        'avg_ClassI_HLA_ethcat5': np.mean(HLA_classI_ethcat[5]),\n",
    "        'avg_ClassI_HLA_ethcat6': np.mean(HLA_classI_ethcat[6]),\n",
    "        'avg_ClassI_HLA_ethcat7': np.mean(HLA_classI_ethcat[7]),\n",
    "        'HLA_ClassI_total': np.mean(HLA_classI_ethcat[1] + HLA_classI_ethcat[2] + HLA_classI_ethcat[4] +\n",
    "                                    HLA_classI_ethcat[5] + HLA_classI_ethcat[6] + HLA_classI_ethcat[7]),\n",
    "\n",
    "        'avg_DR_HLA_ethcat1': np.mean(HLA_DR_ethcat[1]),\n",
    "        'avg_DR_HLA_ethcat2': np.mean(HLA_DR_ethcat[2]),\n",
    "        'avg_DR_HLA_ethcat4': np.mean(HLA_DR_ethcat[4]),\n",
    "        'avg_DR_HLA_ethcat5': np.mean(HLA_DR_ethcat[5]),\n",
    "        'avg_DR_HLA_ethcat6': np.mean(HLA_DR_ethcat[6]),\n",
    "        'avg_DR_HLA_ethcat7': np.mean(HLA_DR_ethcat[7]),\n",
    "        'HLA_DR_total': np.mean(HLA_DR_ethcat[1] + HLA_DR_ethcat[2] + HLA_DR_ethcat[4] +\n",
    "                                 HLA_DR_ethcat[5] + HLA_DR_ethcat[6] + HLA_DR_ethcat[7]),\n",
    "\n",
    "        'avg_DQ_HLA_ethcat1': np.mean(HLA_DQ_ethcat[1]),\n",
    "        'avg_DQ_HLA_ethcat2': np.mean(HLA_DQ_ethcat[2]),\n",
    "        'avg_DQ_HLA_ethcat4': np.mean(HLA_DQ_ethcat[4]),\n",
    "        'avg_DQ_HLA_ethcat5': np.mean(HLA_DQ_ethcat[5]),\n",
    "        'avg_DQ_HLA_ethcat6': np.mean(HLA_DQ_ethcat[6]),\n",
    "        'avg_DQ_HLA_ethcat7': np.mean(HLA_DQ_ethcat[7]),\n",
    "        'HLA_DQ_total': np.mean(HLA_DQ_ethcat[1] + HLA_DQ_ethcat[2] + HLA_DQ_ethcat[4] +\n",
    "                                 HLA_DQ_ethcat[5] + HLA_DQ_ethcat[6] + HLA_DQ_ethcat[7]),\n",
    "\n",
    "        'F_per_ethcat': F_per_ethcat,\n",
    "        'L_per_ethcat': L_per_ethcat,\n",
    "        'F_total': F_total,\n",
    "        'lr_quality_total': np.mean(lr_quality_ethcat1 + lr_quality_ethcat2 + lr_quality_ethcat4 +\n",
    "                                    lr_quality_ethcat5 + lr_quality_ethcat6 + lr_quality_ethcat7),\n",
    "        'hr_quality_total': np.mean(hr_quality_ethcat1 + hr_quality_ethcat2 + hr_quality_ethcat4 +\n",
    "                                    hr_quality_ethcat5 + hr_quality_ethcat6 + hr_quality_ethcat7),\n",
    "        'e_quality_total': np.mean(e_quality_ethcat1 + e_quality_ethcat2 + e_quality_ethcat4 +\n",
    "                                   e_quality_ethcat5 + e_quality_ethcat6 + e_quality_ethcat7),\n",
    "        'Waiting_time_mean': pd.concat([\n",
    "            df_ethnicity[df_ethnicity['ethnicity'] == e]['waiting time (months)'] for e in [1,2,4,5,6,7]\n",
    "        ]).mean() if not df_ethnicity.empty else np.nan\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(m1, m2, m4, m5, base_seed=12345):\n",
    "    \n",
    "    multipliers_ethnicity = {\n",
    "    1: m1,\n",
    "    2: m2,\n",
    "    4: m4,\n",
    "    5: m5,\n",
    "    6: 1.0,\n",
    "    7: 1.0\n",
    "}\n",
    "\n",
    "    simulations_results = []\n",
    "    for i in range(100):\n",
    "        #print(f\"Runing simulation {i + 1} of 100\")\n",
    "        seed_i = base_seed + i\n",
    "\n",
    "        results = run_simulation(\n",
    "            total_time=10*12, \n",
    "            arrival_rate=(990/(10*12)), \n",
    "            departure_rate=((990/(10*12))*0.29), \n",
    "            match_run=3,\n",
    "            pairs=pairs, \n",
    "            compatibility=compatibility, \n",
    "            hla_lr = hla_lr,\n",
    "            hla_hr=hla_hr,\n",
    "            hla_eplet= hla_eplet,\n",
    "            multipliers_ethnicity= multipliers_ethnicity,\n",
    "            seed=seed_i  \n",
    "        )\n",
    "    \n",
    "        simulations_results.append({\n",
    "            'simulation': i + 1,\n",
    "            'cycles': results['historial_cycles'],\n",
    "            'total_cycles': len(results['historial_cycles']),\n",
    "            'total_departures': len(results['historial_departures']),\n",
    "            'total_entries': results['entries_nodes'],\n",
    "            'arrivals_by_ethcat': results['arrivals_by_ethcat'],\n",
    "            'waiting_time_mean1':   results['waiting_time_mean1'] ,\n",
    "            'waiting_time_mean2':   results['waiting_time_mean2'],\n",
    "            'waiting_time_mean4':   results['waiting_time_mean4'],\n",
    "            'waiting_time_mean5':   results['waiting_time_mean5'],\n",
    "            'waiting_time_mean6':   results['waiting_time_mean6'],\n",
    "            'waiting_time_mean7':   results['waiting_time_mean7'],\n",
    "            'lr_quality_ethcat1': results['lr_quality_ethcat1'],\n",
    "            'lr_quality_ethcat2': results['lr_quality_ethcat2'],\n",
    "            'lr_quality_ethcat4': results['lr_quality_ethcat4'],\n",
    "            'lr_quality_ethcat5': results['lr_quality_ethcat5'],\n",
    "            'lr_quality_ethcat6': results['lr_quality_ethcat6'],\n",
    "            'lr_quality_ethcat7': results['lr_quality_ethcat7'],\n",
    "            'hr_quality_ethcat1': results['hr_quality_ethcat1'],\n",
    "            'hr_quality_ethcat2': results['hr_quality_ethcat2'],\n",
    "            'hr_quality_ethcat4': results['hr_quality_ethcat4'],\n",
    "            'hr_quality_ethcat5': results['hr_quality_ethcat5'],\n",
    "            'hr_quality_ethcat6': results['hr_quality_ethcat6'],\n",
    "            'hr_quality_ethcat7': results['hr_quality_ethcat7'],\n",
    "            'e_quality_ethcat1': results['e_quality_ethcat1'],\n",
    "            'e_quality_ethcat2': results['e_quality_ethcat2'],\n",
    "            'e_quality_ethcat4': results['e_quality_ethcat4'],\n",
    "            'e_quality_ethcat5': results['e_quality_ethcat5'],\n",
    "            'e_quality_ethcat6': results['e_quality_ethcat6'],\n",
    "            'e_quality_ethcat7': results['e_quality_ethcat7'],\n",
    "            'avg_ClassI_HLA_ethcat1': results['avg_ClassI_HLA_ethcat1'],\n",
    "            'avg_ClassI_HLA_ethcat2': results['avg_ClassI_HLA_ethcat2'],\n",
    "            'avg_ClassI_HLA_ethcat4': results['avg_ClassI_HLA_ethcat4'],\n",
    "            'avg_ClassI_HLA_ethcat5': results['avg_ClassI_HLA_ethcat5'],\n",
    "            'avg_ClassI_HLA_ethcat6': results['avg_ClassI_HLA_ethcat6'],\n",
    "            'avg_ClassI_HLA_ethcat7': results['avg_ClassI_HLA_ethcat7'],\n",
    "            'HLA_ClassI_total': results['HLA_ClassI_total'],\n",
    "\n",
    "            'avg_DR_HLA_ethcat1': results['avg_DR_HLA_ethcat1'],\n",
    "            'avg_DR_HLA_ethcat2': results['avg_DR_HLA_ethcat2'],\n",
    "            'avg_DR_HLA_ethcat4': results['avg_DR_HLA_ethcat4'],\n",
    "            'avg_DR_HLA_ethcat5': results['avg_DR_HLA_ethcat5'],\n",
    "            'avg_DR_HLA_ethcat6': results['avg_DR_HLA_ethcat6'],\n",
    "            'avg_DR_HLA_ethcat7': results['avg_DR_HLA_ethcat7'],\n",
    "            'HLA_DR_total': results['HLA_DR_total'],\n",
    "            \n",
    "\n",
    "            'avg_DQ_HLA_ethcat1': results['avg_DQ_HLA_ethcat1'],\n",
    "            'avg_DQ_HLA_ethcat2': results['avg_DQ_HLA_ethcat2'],\n",
    "            'avg_DQ_HLA_ethcat4': results['avg_DQ_HLA_ethcat4'],\n",
    "            'avg_DQ_HLA_ethcat5': results['avg_DQ_HLA_ethcat5'],\n",
    "            'avg_DQ_HLA_ethcat6': results['avg_DQ_HLA_ethcat6'],\n",
    "            'avg_DQ_HLA_ethcat7': results['avg_DQ_HLA_ethcat7'],\n",
    "            'HLA_DQ_total': results['HLA_DQ_total'],\n",
    "\n",
    "            'F_per_ethcat':   results['F_per_ethcat'],   \n",
    "            'L_per_ethcat':   results['L_per_ethcat'],   \n",
    "            'F_total': results['F_total'],\n",
    "            'lr_quality_total': results['lr_quality_total'],\n",
    "            'hr_quality_total': results['hr_quality_total'],\n",
    "            'e_quality_total': results['e_quality_total'],\n",
    "            'Waiting_time_mean': results['Waiting_time_mean']\n",
    "        })\n",
    "        \n",
    "        \n",
    "    df_results = pd.DataFrame(simulations_results)\n",
    "    z = 10 #HERE change the z. 10 for LR and HR, 138 for eplet.\n",
    "    F_P = np.mean(df_results['F_total'])\n",
    "    HLA_P = np.mean(df_results['lr_quality_total']) /z #HERE change the resolution\n",
    "    \n",
    "    results_ethnicity = []\n",
    "    \n",
    "\n",
    "    for index, row in df_results.iterrows():\n",
    "        nodes = set(nodo for ciclo in row['cycles'] for nodo in ciclo)\n",
    "        df_nodes = recipients[recipients['Nodo'].isin(nodes)]\n",
    "        count_ethnicity = df_nodes['ETHCAT'].value_counts().to_dict()\n",
    "        results_ethnicity .append({'simulation': row['simulation'], **count_ethnicity})\n",
    "\n",
    "    df_results_ethnicity  = pd.DataFrame(results_ethnicity ).fillna(0)\n",
    "    arrivals_by_ethcat = pd.DataFrame(df_results['arrivals_by_ethcat'].tolist())\n",
    "    arrivals_by_ethcat_mean = arrivals_by_ethcat.mean()\n",
    "    target_ethnicity = [1, 2, 4, 5, 6, 7]\n",
    "    m = sum(arrivals_by_ethcat_mean[e] for e in target_ethnicity) \n",
    "    target_ethnicity = [1, 2, 4, 5]\n",
    "    transplant_by_ethcat_mean = df_results_ethnicity.mean()\n",
    "    percentage_by_ethnicity = (transplant_by_ethcat_mean) / arrivals_by_ethcat_mean\n",
    "    quality = df_results[\n",
    "        ['lr_quality_ethcat1', 'lr_quality_ethcat2', 'lr_quality_ethcat4', 'lr_quality_ethcat5'] #HERE change the resolution\n",
    "    ].mean()\n",
    "    quality = quality / z\n",
    "\n",
    "    kpi_total = 0\n",
    "    rows = []\n",
    "\n",
    "    for e in target_ethnicity:\n",
    "        \n",
    "        s = arrivals_by_ethcat_mean[e]\n",
    "        F_s = percentage_by_ethnicity[e]\n",
    "        HLA_s = quality[f'lr_quality_ethcat{e}'] #HERE change the resolution\n",
    "        ######\n",
    "        #F_diff = (s / m)*(F_s - F_P)\n",
    "        #HLA_diff = (s / m)*((1/m)*(HLA_s - HLA_P))\n",
    "\n",
    "        F_diff = F_s - F_P\n",
    "        HLA_diff = (HLA_s - HLA_P)/m\n",
    "        #####\n",
    "\n",
    "        term = (s / m) * (abs(F_s - F_P) + (1/m)*abs(HLA_s - HLA_P))\n",
    "        #term = (s / m) * (abs(F_s - F_P) + abs(HLA_s - HLA_P))\n",
    "\n",
    "        kpi_total = kpi_total + term\n",
    "        print(f\"For ethcat {e} : s = {s}, m = {m}, F_s = {F_s}, F_P = {F_P}, HLA_s = {HLA_s}, HLA_P = {HLA_P} \")\n",
    "\n",
    "        rows.append({\n",
    "        \"ethcat\": e,\n",
    "        \"kpi_total\": term,      \n",
    "        \"F_diff\": F_diff,         \n",
    "        \"HLA_diff\": HLA_diff    \n",
    "    })\n",
    "\n",
    "    tabla_kpi = pd.DataFrame(rows).set_index(\"ethcat\")\n",
    "    print(\"KPI:\",kpi_total)\n",
    "    print(\"\\nResumen KPI por etnia (fila = etnia):\")\n",
    "    print(tabla_kpi.to_string(float_format=lambda x: f\"{x:.6f}\"))\n",
    "\n",
    "    return tabla_kpi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.6987625938197309, F_P = 0.6709550906277216, HLA_s = 0.470233449273064, HLA_P = 0.45119669872974166 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.6408115260217583, F_P = 0.6709550906277216, HLA_s = 0.41555242105152573, HLA_P = 0.45119669872974166 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.623013177557397, F_P = 0.6709550906277216, HLA_s = 0.43156936271385027, HLA_P = 0.45119669872974166 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.6085698788645814, F_P = 0.6709550906277216, HLA_s = 0.3937720627235929, HLA_P = 0.45119669872974166 \n",
      "KPI: 0.03283010246464941\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.016831  0.027808  0.000019\n",
      "2        0.005248 -0.030144 -0.000036\n",
      "4        0.007220 -0.047942 -0.000020\n",
      "5        0.003531 -0.062385 -0.000059\n"
     ]
    }
   ],
   "source": [
    "tabla = objective_function(1,1,1,1.000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kpi_total</th>\n",
       "      <th>F_diff</th>\n",
       "      <th>HLA_diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethcat</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016945</td>\n",
       "      <td>0.027995</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006334</td>\n",
       "      <td>-0.036389</td>\n",
       "      <td>-0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008173</td>\n",
       "      <td>-0.054276</td>\n",
       "      <td>-0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001737</td>\n",
       "      <td>-0.030649</td>\n",
       "      <td>-0.000057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        kpi_total    F_diff  HLA_diff\n",
       "ethcat                               \n",
       "1        0.016945  0.027995  0.000020\n",
       "2        0.006334 -0.036389 -0.000037\n",
       "4        0.008173 -0.054276 -0.000021\n",
       "5        0.001737 -0.030649 -0.000057"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTION 1 SEARCH JUST WITH \"F\" DIFFERENCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.6639225099736291, F_P = 0.639818133553713, HLA_s = 0.7670647309753491, HLA_P = 0.761172272632183 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.6397530138194648, F_P = 0.639818133553713, HLA_s = 0.7532620420466656, HLA_P = 0.761172272632183 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.5846352397772041, F_P = 0.639818133553713, HLA_s = 0.7512880067846092, HLA_P = 0.761172272632183 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.538239016452721, F_P = 0.639818133553713, HLA_s = 0.7414409854376514, HLA_P = 0.761172272632183 \n",
      "KPI: 0.028650185956190802\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.014583  0.024104  0.000006\n",
      "2        0.000013 -0.000065 -0.000008\n",
      "4        0.008308 -0.055183 -0.000010\n",
      "5        0.005746 -0.101579 -0.000020\n",
      "[init] KPI = 0.028650  m = {1: 1.0, 2: 1.0, 4: 1.0, 5: 1.0}  | base_seed=12345\n",
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.6644465481100819, F_P = 0.6401835744021828, HLA_s = 0.7672599696527567, HLA_P = 0.7606166455730251 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.637400764481035, F_P = 0.6401835744021828, HLA_s = 0.7518614335357275, HLA_P = 0.7606166455730251 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.5768917266675724, F_P = 0.6401835744021828, HLA_s = 0.7494643251134226, HLA_P = 0.7606166455730251 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.5664436810703308, F_P = 0.6401835744021828, HLA_s = 0.7396312034309857, HLA_P = 0.7606166455730251 \n",
      "KPI: 0.028865765203478912\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.014680  0.024263  0.000007\n",
      "2        0.000485 -0.002783 -0.000009\n",
      "4        0.009529 -0.063292 -0.000011\n",
      "5        0.004171 -0.073740 -0.000021\n",
      "[iter 1] give step=0.001 to ethcat 5 | KPI 0.028650 → 0.028866 | m={1: 1.0, 2: 1.0, 4: 1.0, 5: 1.001}\n",
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.6606768544188248, F_P = 0.6401020576861771, HLA_s = 0.7669403338695716, HLA_P = 0.7604243196470365 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.6388709203175537, F_P = 0.6401020576861771, HLA_s = 0.7521232930946156, HLA_P = 0.7604243196470365 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.5766200244531993, F_P = 0.6401020576861771, HLA_s = 0.7501451736525481, HLA_P = 0.7604243196470365 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.6020611101066714, F_P = 0.6401020576861771, HLA_s = 0.7387444421320171, HLA_P = 0.7604243196470365 \n",
      "KPI: 0.024374607023753884\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.012449  0.020575  0.000007\n",
      "2        0.000216 -0.001231 -0.000008\n",
      "4        0.009558 -0.063482 -0.000011\n",
      "5        0.002153 -0.038041 -0.000022\n",
      "[iter 2] give step=0.001 to ethcat 5 | KPI 0.028866 → 0.024375 | m={1: 1.0, 2: 1.0, 4: 1.0, 5: 1.0019999999999998}\n",
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.6568395429035094, F_P = 0.6422137806864865, HLA_s = 0.7665147680588943, HLA_P = 0.7599548961868957 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.63228462216995, F_P = 0.6422137806864865, HLA_s = 0.7520704953764407, HLA_P = 0.7599548961868957 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.6153375900013586, F_P = 0.6422137806864865, HLA_s = 0.7498147977402739, HLA_P = 0.7599548961868957 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.5986259265955524, F_P = 0.6422137806864865, HLA_s = 0.7369104534827409, HLA_P = 0.7599548961868957 \n",
      "KPI: 0.01709204879351705\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.008851  0.014626  0.000007\n",
      "2        0.001728 -0.009929 -0.000008\n",
      "4        0.004047 -0.026876 -0.000010\n",
      "5        0.002466 -0.043588 -0.000024\n",
      "[iter 3] give step=0.001 to ethcat 4 | KPI 0.024375 → 0.017092 | m={1: 1.0, 2: 1.0, 4: 1.001, 5: 1.0019999999999998}\n",
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.6546419636216108, F_P = 0.6421807697382269, HLA_s = 0.7652005763480368, HLA_P = 0.7587435864671705 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.6331667156718612, F_P = 0.6421807697382269, HLA_s = 0.7504886483936102, HLA_P = 0.7587435864671705 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.6135036000543403, F_P = 0.6421807697382269, HLA_s = 0.750273671962279, HLA_P = 0.7587435864671705 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.6228530103055505, F_P = 0.6421807697382269, HLA_s = 0.7354479357722116, HLA_P = 0.7587435864671705 \n",
      "KPI: 0.014522539001619704\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.007541  0.012461  0.000007\n",
      "2        0.001569 -0.009014 -0.000008\n",
      "4        0.004318 -0.028677 -0.000009\n",
      "5        0.001094 -0.019328 -0.000024\n",
      "[iter 4] give step=0.001 to ethcat 5 | KPI 0.017092 → 0.014523 | m={1: 1.0, 2: 1.0, 4: 1.001, 5: 1.0029999999999997}\n",
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.6477280411116371, F_P = 0.6410437320728952, HLA_s = 0.7649248007410654, HLA_P = 0.7579492180212728 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.631814172302264, F_P = 0.6410437320728952, HLA_s = 0.749123757749503, HLA_P = 0.7579492180212728 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.6394511615269665, F_P = 0.6410437320728952, HLA_s = 0.7478245406802951, HLA_P = 0.7579492180212728 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.616886638944133, F_P = 0.6410437320728952, HLA_s = 0.7364439349462479, HLA_P = 0.7579492180212728 \n",
      "KPI: 0.007262438617279949\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.004047  0.006684  0.000007\n",
      "2        0.001606 -0.009230 -0.000009\n",
      "4        0.000241 -0.001593 -0.000010\n",
      "5        0.001367 -0.024157 -0.000022\n",
      "[iter 5] give step=0.001 to ethcat 4 | KPI 0.014523 → 0.007262 | m={1: 1.0, 2: 1.0, 4: 1.0019999999999998, 5: 1.0029999999999997}\n",
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.6475251876394619, F_P = 0.6408559243254439, HLA_s = 0.7638999286329409, HLA_P = 0.7571935155185542 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.6279917671273155, F_P = 0.6408559243254439, HLA_s = 0.7493297480529523, HLA_P = 0.7571935155185542 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.6370737671512023, F_P = 0.6408559243254439, HLA_s = 0.7476136198744476, HLA_P = 0.7571935155185542 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.6322545651780871, F_P = 0.6408559243254439, HLA_s = 0.734448718815941, HLA_P = 0.7571935155185542 \n",
      "KPI: 0.007334814370490039\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.004038  0.006669  0.000007\n",
      "2        0.002238 -0.012864 -0.000008\n",
      "4        0.000571 -0.003782 -0.000010\n",
      "5        0.000488 -0.008601 -0.000023\n",
      "[iter 6] give step=0.001 to ethcat 5 | KPI 0.007262 → 0.007335 | m={1: 1.0, 2: 1.0, 4: 1.0019999999999998, 5: 1.0039999999999996}\n",
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.6418960037865983, F_P = 0.6400371891813761, HLA_s = 0.7633686675267182, HLA_P = 0.7567520492478028 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.6516318729785356, F_P = 0.6400371891813761, HLA_s = 0.7495054521685475, HLA_P = 0.7567520492478028 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.626884934112213, F_P = 0.6400371891813761, HLA_s = 0.7476141688772742, HLA_P = 0.7567520492478028 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.6322545651780871, F_P = 0.6400371891813761, HLA_s = 0.7330330262619225, HLA_P = 0.7567520492478028 \n",
      "KPI: 0.005568437255434394\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.001128  0.001859  0.000007\n",
      "2        0.002017  0.011595 -0.000007\n",
      "4        0.001981 -0.013152 -0.000009\n",
      "5        0.000442 -0.007783 -0.000024\n",
      "[iter 7] give step=0.001 to ethcat 2 | KPI 0.007335 → 0.005568 | m={1: 1.0, 2: 1.001, 4: 1.0019999999999998, 5: 1.0039999999999996}\n",
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.635996348637501, F_P = 0.6377589214604411, HLA_s = 0.7626828031796029, HLA_P = 0.7559800218634394 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.6394589826521611, F_P = 0.6377589214604411, HLA_s = 0.7487777113450674, HLA_P = 0.7559800218634394 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.652832495584839, F_P = 0.6377589214604411, HLA_s = 0.7457046627522114, HLA_P = 0.7559800218634394 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.6252033990236846, F_P = 0.6377589214604411, HLA_s = 0.7352419632123562, HLA_P = 0.7559800218634394 \n",
      "KPI: 0.004349001153376753\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.001070 -0.001763  0.000007\n",
      "2        0.000297  0.001700 -0.000007\n",
      "4        0.002271  0.015074 -0.000011\n",
      "5        0.000711 -0.012556 -0.000021\n",
      "[iter 8] give step=0.001 to ethcat 4 | KPI 0.005568 → 0.004349 | m={1: 1.0, 2: 1.001, 4: 1.0029999999999997, 5: 1.0039999999999996}\n",
      "[iter 9] Next step would revisit a previous state. Stop.\n",
      "Best m: {1: 1.0, 2: 1.001, 4: 1.0029999999999997, 5: 1.0039999999999996}\n",
      "Best KPI: 0.004349001153376753\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.001070 -0.001763  0.000007\n",
      "2        0.000297  0.001700 -0.000007\n",
      "4        0.002271  0.015074 -0.000011\n",
      "5        0.000711 -0.012556 -0.000021\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _kpi_total_from_table(tabla: pd.DataFrame) -> float:\n",
    "    #Sum of per-ethnicity KPI. \n",
    "    if 'kpi_total' not in tabla.columns:\n",
    "        raise ValueError(\"The table must have a 'kpi_total' column.\")\n",
    "    return float(tabla['kpi_total'].sum())\n",
    "\n",
    "def _get_F_col(tabla: pd.DataFrame) -> str:\n",
    "    #Return F-diff column \n",
    "    if 'F_diff' in tabla.columns:\n",
    "        return 'F_diff'\n",
    "    if 'F_s - F_P' in tabla.columns:\n",
    "        return 'F_s - F_P'\n",
    "    raise ValueError(\"The table must have an F column: 'F_diff' or 'F_s - F_P'.\")\n",
    "\n",
    "def _state_key(m: dict, round_decimals: int = 6):\n",
    "    #key for the multiplier state (rounded to avoid float noise)\n",
    "    return tuple(round(m[e], round_decimals) for e in (1,2,4,5))\n",
    "\n",
    "def _eval_state(m: dict, base_seed: int, cache: dict, state_round: int):\n",
    "    #Evaluate objective_function\n",
    "    key = _state_key(m, state_round)\n",
    "    if key in cache:\n",
    "        return cache[key]\n",
    "    tabla = objective_function(m[1], m[2], m[4], m[5], base_seed=base_seed)\n",
    "    kpi = _kpi_total_from_table(tabla)\n",
    "    cache[key] = (tabla, kpi)\n",
    "    return tabla, kpi\n",
    "\n",
    "# algorithm\n",
    "def single_push_search(step=0.001, max_iters=200, f_zero_tol=1e-12, state_round=6, base_seed=12345, verbose=True):\n",
    "    \"\"\"\n",
    "     - Pick ethnicity with largest |F_diff|.\n",
    "      - If F_diff > 0 → decrease its multiplier by 'step'; if < 0 → increase it.\n",
    "      - Stop if the next state was already visited.\n",
    "      - Return the best state (lowest KPI) seen.\n",
    "    \"\"\"\n",
    "    # Initial multipliers\n",
    "    m = {1:1.0, 2:1.0, 4:1.0, 5:1.0}\n",
    "\n",
    "    cache = {}\n",
    "    tabla, kpi = _eval_state(m, base_seed, cache, state_round)\n",
    "    F_col = _get_F_col(tabla)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[init] KPI = {kpi:.6f}  m = {m}  | base_seed={base_seed}\")\n",
    "\n",
    "    history = [(0, m.copy(), kpi)]\n",
    "    seen = { _state_key(m, state_round) }\n",
    "\n",
    "    best_m, best_kpi, best_table = m.copy(), kpi, tabla.copy()\n",
    "\n",
    "    for it in range(1, max_iters+1):\n",
    "        # Select ethnicity with the largest absolute F deviation\n",
    "        F_series = tabla[F_col].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        if F_series.empty or F_series.abs().max() <= f_zero_tol:\n",
    "            if verbose:\n",
    "                print(f\"[iter {it}] No useful signal in F (all ~0). Stop.\")\n",
    "            break\n",
    "\n",
    "        eth_star = int(F_series.abs().idxmax())\n",
    "        F_val = float(F_series.loc[eth_star])\n",
    "\n",
    "        # Direction: positive → take, negative → give\n",
    "        step_dir = -1.0 if F_val > 0.0 else (1.0 if F_val < 0.0 else 0.0)\n",
    "        if step_dir == 0.0:\n",
    "            if verbose:\n",
    "                print(f\"[iter {it}] Top |F| is exactly 0. Stop.\")\n",
    "            break\n",
    "\n",
    "        proposed = m.copy()\n",
    "        proposed[eth_star] += step_dir * float(step)\n",
    "\n",
    "        # Stop if this state was already explored\n",
    "        key = _state_key(proposed, state_round)\n",
    "        if key in seen:\n",
    "            if verbose:\n",
    "                print(f\"[iter {it}] Next step would revisit a previous state. Stop.\")\n",
    "            break\n",
    "\n",
    "        # Evaluate proposed state\n",
    "        new_tabla, new_kpi = _eval_state(proposed, base_seed, cache, state_round)\n",
    "\n",
    "        if verbose:\n",
    "            action = \"give\" if step_dir > 0 else \"take\"\n",
    "            print(f\"[iter {it}] {action} step={step:.3f} to ethcat {eth_star} \"\n",
    "                  f\"| KPI {kpi:.6f} → {new_kpi:.6f} | m={proposed}\")\n",
    "\n",
    "        m, tabla, kpi = proposed, new_tabla, new_kpi\n",
    "        F_col = _get_F_col(tabla)\n",
    "        seen.add(key)\n",
    "        history.append((it, m.copy(), kpi))\n",
    "\n",
    "        # actualizar mejor\n",
    "        if new_kpi < best_kpi:\n",
    "            best_kpi = new_kpi\n",
    "            best_m = m.copy()\n",
    "            best_table = tabla.copy()\n",
    "\n",
    "    return {\n",
    "        \"best_multipliers\": best_m,\n",
    "        \"best_kpi\": best_kpi,\n",
    "        \"best_table\": best_table,\n",
    "        \"last_multipliers\": m,        \n",
    "        \"last_kpi\": kpi,\n",
    "        \"last_table\": tabla,\n",
    "        \"history\": history,           \n",
    "        \"visited_count\": len(seen),\n",
    "    }\n",
    "\n",
    "res = single_push_search(step=0.001, max_iters=200, base_seed=12345, f_zero_tol=1e-12, state_round=6, verbose=True)\n",
    "print(\"Best m:\", res[\"best_multipliers\"])\n",
    "print(\"Best KPI:\", res[\"best_kpi\"])\n",
    "print(res[\"best_table\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTION 2 SEARCH WITH \"F\" DIFFERENCE AND \"HLA\" DIFFERENCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.6639225099736291, F_P = 0.639818133553713, HLA_s = 0.7670647309753491, HLA_P = 0.761172272632183 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.6397530138194648, F_P = 0.639818133553713, HLA_s = 0.7532620420466656, HLA_P = 0.761172272632183 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.5846352397772041, F_P = 0.639818133553713, HLA_s = 0.7512880067846092, HLA_P = 0.761172272632183 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.538239016452721, F_P = 0.639818133553713, HLA_s = 0.7414409854376514, HLA_P = 0.761172272632183 \n",
      "KPI: 0.028650185956190802\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.014583  0.024104  0.000006\n",
      "2        0.000013 -0.000065 -0.000008\n",
      "4        0.008308 -0.055183 -0.000010\n",
      "5        0.005746 -0.101579 -0.000020\n",
      "[init] KPI = 0.028650  m = {1: 1.0, 2: 1.0, 4: 1.0, 5: 1.0}  | base_seed=12345\n",
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.6644465481100819, F_P = 0.6401835744021828, HLA_s = 0.7672599696527567, HLA_P = 0.7606166455730251 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.637400764481035, F_P = 0.6401835744021828, HLA_s = 0.7518614335357275, HLA_P = 0.7606166455730251 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.5768917266675724, F_P = 0.6401835744021828, HLA_s = 0.7494643251134226, HLA_P = 0.7606166455730251 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.5664436810703308, F_P = 0.6401835744021828, HLA_s = 0.7396312034309857, HLA_P = 0.7606166455730251 \n",
      "KPI: 0.028865765203478912\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.014680  0.024263  0.000007\n",
      "2        0.000485 -0.002783 -0.000009\n",
      "4        0.009529 -0.063292 -0.000011\n",
      "5        0.004171 -0.073740 -0.000021\n",
      "[iter 1] give step=0.001 to ethcat 5 (signal=F, diff=-0.101579) | KPI 0.028650 → 0.028866 | m={1: 1.0, 2: 1.0, 4: 1.0, 5: 1.001}\n",
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.6606768544188248, F_P = 0.6401020576861771, HLA_s = 0.7669403338695716, HLA_P = 0.7604243196470365 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.6388709203175537, F_P = 0.6401020576861771, HLA_s = 0.7521232930946156, HLA_P = 0.7604243196470365 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.5766200244531993, F_P = 0.6401020576861771, HLA_s = 0.7501451736525481, HLA_P = 0.7604243196470365 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.6020611101066714, F_P = 0.6401020576861771, HLA_s = 0.7387444421320171, HLA_P = 0.7604243196470365 \n",
      "KPI: 0.024374607023753884\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.012449  0.020575  0.000007\n",
      "2        0.000216 -0.001231 -0.000008\n",
      "4        0.009558 -0.063482 -0.000011\n",
      "5        0.002153 -0.038041 -0.000022\n",
      "[iter 2] give step=0.001 to ethcat 5 (signal=F, diff=-0.073740) | KPI 0.028866 → 0.024375 | m={1: 1.0, 2: 1.0, 4: 1.0, 5: 1.0019999999999998}\n",
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.6568395429035094, F_P = 0.6422137806864865, HLA_s = 0.7665147680588943, HLA_P = 0.7599548961868957 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.63228462216995, F_P = 0.6422137806864865, HLA_s = 0.7520704953764407, HLA_P = 0.7599548961868957 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.6153375900013586, F_P = 0.6422137806864865, HLA_s = 0.7498147977402739, HLA_P = 0.7599548961868957 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.5986259265955524, F_P = 0.6422137806864865, HLA_s = 0.7369104534827409, HLA_P = 0.7599548961868957 \n",
      "KPI: 0.01709204879351705\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.008851  0.014626  0.000007\n",
      "2        0.001728 -0.009929 -0.000008\n",
      "4        0.004047 -0.026876 -0.000010\n",
      "5        0.002466 -0.043588 -0.000024\n",
      "[iter 3] give step=0.001 to ethcat 4 (signal=F, diff=-0.063482) | KPI 0.024375 → 0.017092 | m={1: 1.0, 2: 1.0, 4: 1.001, 5: 1.0019999999999998}\n",
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.6546419636216108, F_P = 0.6421807697382269, HLA_s = 0.7652005763480368, HLA_P = 0.7587435864671705 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.6331667156718612, F_P = 0.6421807697382269, HLA_s = 0.7504886483936102, HLA_P = 0.7587435864671705 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.6135036000543403, F_P = 0.6421807697382269, HLA_s = 0.750273671962279, HLA_P = 0.7587435864671705 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.6228530103055505, F_P = 0.6421807697382269, HLA_s = 0.7354479357722116, HLA_P = 0.7587435864671705 \n",
      "KPI: 0.014522539001619704\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.007541  0.012461  0.000007\n",
      "2        0.001569 -0.009014 -0.000008\n",
      "4        0.004318 -0.028677 -0.000009\n",
      "5        0.001094 -0.019328 -0.000024\n",
      "[iter 4] give step=0.001 to ethcat 5 (signal=F, diff=-0.043588) | KPI 0.017092 → 0.014523 | m={1: 1.0, 2: 1.0, 4: 1.001, 5: 1.0029999999999997}\n",
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.6477280411116371, F_P = 0.6410437320728952, HLA_s = 0.7649248007410654, HLA_P = 0.7579492180212728 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.631814172302264, F_P = 0.6410437320728952, HLA_s = 0.749123757749503, HLA_P = 0.7579492180212728 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.6394511615269665, F_P = 0.6410437320728952, HLA_s = 0.7478245406802951, HLA_P = 0.7579492180212728 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.616886638944133, F_P = 0.6410437320728952, HLA_s = 0.7364439349462479, HLA_P = 0.7579492180212728 \n",
      "KPI: 0.007262438617279949\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.004047  0.006684  0.000007\n",
      "2        0.001606 -0.009230 -0.000009\n",
      "4        0.000241 -0.001593 -0.000010\n",
      "5        0.001367 -0.024157 -0.000022\n",
      "[iter 5] give step=0.001 to ethcat 4 (signal=F, diff=-0.028677) | KPI 0.014523 → 0.007262 | m={1: 1.0, 2: 1.0, 4: 1.0019999999999998, 5: 1.0029999999999997}\n",
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.6475251876394619, F_P = 0.6408559243254439, HLA_s = 0.7638999286329409, HLA_P = 0.7571935155185542 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.6279917671273155, F_P = 0.6408559243254439, HLA_s = 0.7493297480529523, HLA_P = 0.7571935155185542 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.6370737671512023, F_P = 0.6408559243254439, HLA_s = 0.7476136198744476, HLA_P = 0.7571935155185542 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.6322545651780871, F_P = 0.6408559243254439, HLA_s = 0.734448718815941, HLA_P = 0.7571935155185542 \n",
      "KPI: 0.007334814370490039\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.004038  0.006669  0.000007\n",
      "2        0.002238 -0.012864 -0.000008\n",
      "4        0.000571 -0.003782 -0.000010\n",
      "5        0.000488 -0.008601 -0.000023\n",
      "[iter 6] give step=0.001 to ethcat 5 (signal=F, diff=-0.024157) | KPI 0.007262 → 0.007335 | m={1: 1.0, 2: 1.0, 4: 1.0019999999999998, 5: 1.0039999999999996}\n",
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.6418960037865983, F_P = 0.6400371891813761, HLA_s = 0.7633686675267182, HLA_P = 0.7567520492478028 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.6516318729785356, F_P = 0.6400371891813761, HLA_s = 0.7495054521685475, HLA_P = 0.7567520492478028 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.626884934112213, F_P = 0.6400371891813761, HLA_s = 0.7476141688772742, HLA_P = 0.7567520492478028 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.6322545651780871, F_P = 0.6400371891813761, HLA_s = 0.7330330262619225, HLA_P = 0.7567520492478028 \n",
      "KPI: 0.005568437255434394\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.001128  0.001859  0.000007\n",
      "2        0.002017  0.011595 -0.000007\n",
      "4        0.001981 -0.013152 -0.000009\n",
      "5        0.000442 -0.007783 -0.000024\n",
      "[iter 7] give step=0.001 to ethcat 2 (signal=F, diff=-0.012864) | KPI 0.007335 → 0.005568 | m={1: 1.0, 2: 1.001, 4: 1.0019999999999998, 5: 1.0039999999999996}\n",
      "For ethcat 1 : s = 591.56, m = 978.0099999999999, F_s = 0.635996348637501, F_P = 0.6377589214604411, HLA_s = 0.7626828031796029, HLA_P = 0.7559800218634394 \n",
      "For ethcat 2 : s = 170.05, m = 978.0099999999999, F_s = 0.6394589826521611, F_P = 0.6377589214604411, HLA_s = 0.7487777113450674, HLA_P = 0.7559800218634394 \n",
      "For ethcat 4 : s = 147.22, m = 978.0099999999999, F_s = 0.652832495584839, F_P = 0.6377589214604411, HLA_s = 0.7457046627522114, HLA_P = 0.7559800218634394 \n",
      "For ethcat 5 : s = 55.31, m = 978.0099999999999, F_s = 0.6252033990236846, F_P = 0.6377589214604411, HLA_s = 0.7352419632123562, HLA_P = 0.7559800218634394 \n",
      "KPI: 0.004349001153376753\n",
      "\n",
      "Resumen KPI por etnia (fila = etnia):\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.001070 -0.001763  0.000007\n",
      "2        0.000297  0.001700 -0.000007\n",
      "4        0.002271  0.015074 -0.000011\n",
      "5        0.000711 -0.012556 -0.000021\n",
      "[iter 8] give step=0.001 to ethcat 4 (signal=F, diff=-0.013152) | KPI 0.005568 → 0.004349 | m={1: 1.0, 2: 1.001, 4: 1.0029999999999997, 5: 1.0039999999999996}\n",
      "[iter 9] Next step would revisit a previous state. Stop.\n",
      "Best m: {1: 1.0, 2: 1.001, 4: 1.0029999999999997, 5: 1.0039999999999996}\n",
      "Best KPI: 0.004349001153376753\n",
      "        kpi_total    F_diff  HLA_diff\n",
      "ethcat                               \n",
      "1        0.001070 -0.001763  0.000007\n",
      "2        0.000297  0.001700 -0.000007\n",
      "4        0.002271  0.015074 -0.000011\n",
      "5        0.000711 -0.012556 -0.000021\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _kpi_total_from_table(tabla: pd.DataFrame) -> float:\n",
    "    #Sum of per-ethnicity KPI. \n",
    "    if 'kpi_total' not in tabla.columns:\n",
    "        raise ValueError(\"The table must have a 'kpi_total' column.\")\n",
    "    return float(tabla['kpi_total'].sum())\n",
    "\n",
    "def _get_F_col(tabla: pd.DataFrame) -> str:\n",
    "    #Resolve the F-difference column\n",
    "    if 'F_diff' in tabla.columns: return 'F_diff'\n",
    "    if 'F_s - F_P' in tabla.columns: return 'F_s - F_P'\n",
    "    raise ValueError(\"The table must have an F column: 'F_diff' or 'F_s - F_P'.\")\n",
    "\n",
    "def _get_H_col(tabla: pd.DataFrame) -> str:\n",
    "    # Resolve the HLA-difference column\n",
    "    if 'HLA_diff' in tabla.columns: return 'HLA_diff'\n",
    "    if 'HLA_s - HLA_P' in tabla.columns: return 'HLA_s - HLA_P'\n",
    "    raise ValueError(\"The table must have an HLA column: 'HLA_diff' or 'HLA_s - HLA_P'.\")\n",
    "\n",
    "def _state_key(m: dict, round_decimals: int = 6):\n",
    "    return tuple(round(m[e], round_decimals) for e in (1,2,4,5))\n",
    "\n",
    "def _eval_state(m: dict, base_seed: int, cache: dict, state_round: int):\n",
    "    key = _state_key(m, state_round)\n",
    "    if key in cache:\n",
    "        return cache[key]\n",
    "    # objective_function \n",
    "    tabla = objective_function(m[1], m[2], m[4], m[5], base_seed=base_seed)\n",
    "    kpi = _kpi_total_from_table(tabla)\n",
    "    cache[key] = (tabla, kpi)\n",
    "    return tabla, kpi\n",
    "\n",
    "# algorithm\n",
    "def single_push_search_both(step=0.001, max_iters=200, zero_tol=1e-12,\n",
    "                            state_round=6, base_seed=12345, verbose=True):\n",
    "    \"\"\"\n",
    "      1) Read F and HLA diffs from the table.\n",
    "      2) Pick (ethnicity, signal) with largest |diff|.\n",
    "      3) If diff > 0 → TAKE 'step' from that ethnicity's multiplier.\n",
    "         If diff < 0 → GIVE 'step' to that ethnicity's multiplier.\n",
    "      4) Accept unless the next state was already visited (then stop).\n",
    "    Returns the best (lowest KPI) among all visited states.\n",
    "    \"\"\"\n",
    "    # Initial multipliers\n",
    "    m = {1:1.0, 2:1.0, 4:1.0, 5:1.0}\n",
    "    cache = {}\n",
    "\n",
    "    tabla, kpi = _eval_state(m, base_seed, cache, state_round)\n",
    "    F_col, H_col = _get_F_col(tabla), _get_H_col(tabla)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[init] KPI = {kpi:.6f}  m = {m}  | base_seed={base_seed}\")\n",
    "\n",
    "    history = [(0, m.copy(), kpi)]\n",
    "    seen = { _state_key(m, state_round) }\n",
    "    best_m, best_kpi, best_table = m.copy(), kpi, tabla.copy()\n",
    "\n",
    "    for it in range(1, max_iters+1):\n",
    "        F_series  = tabla[F_col].replace([np.inf, -np.inf], np.nan)\n",
    "        H_series  = tabla[H_col].replace([np.inf, -np.inf], np.nan)\n",
    "        long = pd.concat(\n",
    "            [F_series.rename('F'), H_series.rename('HLA')], axis=1\n",
    "        ).stack().dropna()          # Multiindex: (ethcat, signal)\n",
    "\n",
    "        if long.empty or long.abs().max() <= zero_tol:\n",
    "            if verbose:\n",
    "                print(f\"[iter {it}] No useful signal in F/HLA (all ~0). Stop.\")\n",
    "            break\n",
    "\n",
    "        # # Pick the max absolute deviation (ethnicity, which signal)\n",
    "        idx_star = long.abs().idxmax()   # (ethcat, 'F' or 'HLA')\n",
    "        eth_star, sig_star = int(idx_star[0]), idx_star[1]\n",
    "        diff_val = float(long.loc[idx_star])\n",
    "\n",
    "        # Step direction\n",
    "        step_dir = -1.0 if diff_val > 0.0 else (1.0 if diff_val < 0.0 else 0.0)\n",
    "        if step_dir == 0.0:\n",
    "            if verbose:\n",
    "                print(f\"[iter {it}] Top |diff| is exactly 0. Stop.\")\n",
    "            break\n",
    "\n",
    "        # Propose next multipliers\n",
    "        proposed = m.copy()\n",
    "        proposed[eth_star] += step_dir * float(step)\n",
    "\n",
    "        # Stop if we would revisit a previous state\n",
    "        key = _state_key(proposed, state_round)\n",
    "        if key in seen:\n",
    "            if verbose:\n",
    "                print(f\"[iter {it}] Next step would revisit a previous state. Stop.\")\n",
    "            break\n",
    "\n",
    "        # Evaluate proposed state\n",
    "        new_tabla, new_kpi = _eval_state(proposed, base_seed, cache, state_round)\n",
    "\n",
    "        if verbose:\n",
    "            action = \"give\" if step_dir > 0 else \"take\"\n",
    "            print(f\"[iter {it}] {action} step={step:.3f} to ethcat {eth_star} (signal={sig_star}, diff={diff_val:+.6f}) \"\n",
    "                  f\"| KPI {kpi:.6f} → {new_kpi:.6f} | m={proposed}\")\n",
    "\n",
    "        # Accept, record, and update best if improved\n",
    "        m, tabla, kpi = proposed, new_tabla, new_kpi\n",
    "        F_col, H_col = _get_F_col(tabla), _get_H_col(tabla)\n",
    "        seen.add(key)\n",
    "        history.append((it, m.copy(), kpi))\n",
    "\n",
    "        if new_kpi < best_kpi:\n",
    "            best_kpi = new_kpi\n",
    "            best_m = m.copy()\n",
    "            best_table = tabla.copy()\n",
    "\n",
    "    return {\n",
    "        \"best_multipliers\": best_m,\n",
    "        \"best_kpi\": best_kpi,\n",
    "        \"best_table\": best_table,\n",
    "        \"last_multipliers\": m,\n",
    "        \"last_kpi\": kpi,\n",
    "        \"last_table\": tabla,\n",
    "        \"history\": history,\n",
    "        \"visited_count\": len(seen),\n",
    "    }\n",
    "\n",
    "res = single_push_search_both(step=0.001, max_iters=200,\n",
    "                              zero_tol=1e-12, state_round=6,\n",
    "                              base_seed=12345, verbose=True)\n",
    "print(\"Best m:\", res[\"best_multipliers\"])\n",
    "print(\"Best KPI:\", res[\"best_kpi\"])\n",
    "print(res[\"best_table\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After of knowing m1, m2, m3 and m4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runing simulation 1 of 100\n",
      "Runing simulation 2 of 100\n",
      "Runing simulation 3 of 100\n",
      "Runing simulation 4 of 100\n",
      "Runing simulation 5 of 100\n",
      "Runing simulation 6 of 100\n",
      "Runing simulation 7 of 100\n",
      "Runing simulation 8 of 100\n",
      "Runing simulation 9 of 100\n",
      "Runing simulation 10 of 100\n",
      "Runing simulation 11 of 100\n",
      "Runing simulation 12 of 100\n",
      "Runing simulation 13 of 100\n",
      "Runing simulation 14 of 100\n",
      "Runing simulation 15 of 100\n",
      "Runing simulation 16 of 100\n",
      "Runing simulation 17 of 100\n",
      "Runing simulation 18 of 100\n",
      "Runing simulation 19 of 100\n",
      "Runing simulation 20 of 100\n",
      "Runing simulation 21 of 100\n",
      "Runing simulation 22 of 100\n",
      "Runing simulation 23 of 100\n",
      "Runing simulation 24 of 100\n",
      "Runing simulation 25 of 100\n",
      "Runing simulation 26 of 100\n",
      "Runing simulation 27 of 100\n",
      "Runing simulation 28 of 100\n",
      "Runing simulation 29 of 100\n",
      "Runing simulation 30 of 100\n",
      "Runing simulation 31 of 100\n",
      "Runing simulation 32 of 100\n",
      "Runing simulation 33 of 100\n",
      "Runing simulation 34 of 100\n",
      "Runing simulation 35 of 100\n",
      "Runing simulation 36 of 100\n",
      "Runing simulation 37 of 100\n",
      "Runing simulation 38 of 100\n",
      "Runing simulation 39 of 100\n",
      "Runing simulation 40 of 100\n",
      "Runing simulation 41 of 100\n",
      "Runing simulation 42 of 100\n",
      "Runing simulation 43 of 100\n",
      "Runing simulation 44 of 100\n",
      "Runing simulation 45 of 100\n",
      "Runing simulation 46 of 100\n",
      "Runing simulation 47 of 100\n",
      "Runing simulation 48 of 100\n",
      "Runing simulation 49 of 100\n",
      "Runing simulation 50 of 100\n",
      "Runing simulation 51 of 100\n",
      "Runing simulation 52 of 100\n",
      "Runing simulation 53 of 100\n",
      "Runing simulation 54 of 100\n",
      "Runing simulation 55 of 100\n",
      "Runing simulation 56 of 100\n",
      "Runing simulation 57 of 100\n",
      "Runing simulation 58 of 100\n",
      "Runing simulation 59 of 100\n",
      "Runing simulation 60 of 100\n",
      "Runing simulation 61 of 100\n",
      "Runing simulation 62 of 100\n",
      "Runing simulation 63 of 100\n",
      "Runing simulation 64 of 100\n",
      "Runing simulation 65 of 100\n",
      "Runing simulation 66 of 100\n",
      "Runing simulation 67 of 100\n",
      "Runing simulation 68 of 100\n",
      "Runing simulation 69 of 100\n",
      "Runing simulation 70 of 100\n",
      "Runing simulation 71 of 100\n",
      "Runing simulation 72 of 100\n",
      "Runing simulation 73 of 100\n",
      "Runing simulation 74 of 100\n",
      "Runing simulation 75 of 100\n",
      "Runing simulation 76 of 100\n",
      "Runing simulation 77 of 100\n",
      "Runing simulation 78 of 100\n",
      "Runing simulation 79 of 100\n",
      "Runing simulation 80 of 100\n",
      "Runing simulation 81 of 100\n",
      "Runing simulation 82 of 100\n",
      "Runing simulation 83 of 100\n",
      "Runing simulation 84 of 100\n",
      "Runing simulation 85 of 100\n",
      "Runing simulation 86 of 100\n",
      "Runing simulation 87 of 100\n",
      "Runing simulation 88 of 100\n",
      "Runing simulation 89 of 100\n",
      "Runing simulation 90 of 100\n",
      "Runing simulation 91 of 100\n",
      "Runing simulation 92 of 100\n",
      "Runing simulation 93 of 100\n",
      "Runing simulation 94 of 100\n",
      "Runing simulation 95 of 100\n",
      "Runing simulation 96 of 100\n",
      "Runing simulation 97 of 100\n",
      "Runing simulation 98 of 100\n",
      "Runing simulation 99 of 100\n",
      "Runing simulation 100 of 100\n"
     ]
    }
   ],
   "source": [
    "#Insert the multipliers found \n",
    "\n",
    "multipliers_ethnicity = {\n",
    "    1: 1,\n",
    "    2: 1.001,\n",
    "    4: 1.003,\n",
    "    5: 1.004,\n",
    "    6: 1.0,\n",
    "    7: 1.0\n",
    "}\n",
    "\n",
    "simulations_results = []\n",
    "#base_seed=12345\n",
    "\n",
    "for i in range(100):\n",
    "    #seed_i = base_seed + i\n",
    "    print(f\"Runing simulation {i + 1} of 100\")\n",
    "    results = run_simulation(\n",
    "        total_time=10*12, \n",
    "        arrival_rate=(990/(10*12)), \n",
    "        departure_rate=((990/(10*12))*0.29), \n",
    "        match_run=3,\n",
    "        pairs=pairs, \n",
    "        compatibility=compatibility, \n",
    "        hla_lr = hla_lr,\n",
    "        hla_hr=hla_hr,\n",
    "        hla_eplet= hla_eplet,\n",
    "        multipliers_ethnicity= multipliers_ethnicity,\n",
    "        #seed=seed_i  \n",
    "    )\n",
    "   \n",
    "    simulations_results.append({\n",
    "        'simulacion': i + 1,\n",
    "        'cycles': results['historial_cycles'],\n",
    "        'total_cycles': len(results['historial_cycles']),\n",
    "        'total_departures': len(results['historial_departures']),\n",
    "        'total_entries': results['entries_nodes'],\n",
    "        'arrivals_by_ethcat': results['arrivals_by_ethcat'],\n",
    "        'waiting_time_mean1':   results['waiting_time_mean1'] ,\n",
    "        'waiting_time_mean2':   results['waiting_time_mean2'],\n",
    "        'waiting_time_mean4':   results['waiting_time_mean4'],\n",
    "        'waiting_time_mean5':   results['waiting_time_mean5'],\n",
    "        'waiting_time_mean6':   results['waiting_time_mean6'],\n",
    "        'waiting_time_mean7':   results['waiting_time_mean7'],\n",
    "        'lr_quality_ethcat1': results['lr_quality_ethcat1'],\n",
    "        'lr_quality_ethcat2': results['lr_quality_ethcat2'],\n",
    "        'lr_quality_ethcat4': results['lr_quality_ethcat4'],\n",
    "        'lr_quality_ethcat5': results['lr_quality_ethcat5'],\n",
    "        'lr_quality_ethcat6': results['lr_quality_ethcat6'],\n",
    "        'lr_quality_ethcat7': results['lr_quality_ethcat7'],\n",
    "        'hr_quality_ethcat1': results['hr_quality_ethcat1'],\n",
    "        'hr_quality_ethcat2': results['hr_quality_ethcat2'],\n",
    "        'hr_quality_ethcat4': results['hr_quality_ethcat4'],\n",
    "        'hr_quality_ethcat5': results['hr_quality_ethcat5'],\n",
    "        'hr_quality_ethcat6': results['hr_quality_ethcat6'],\n",
    "        'hr_quality_ethcat7': results['hr_quality_ethcat7'],\n",
    "        'e_quality_ethcat1': results['e_quality_ethcat1'],\n",
    "        'e_quality_ethcat2': results['e_quality_ethcat2'],\n",
    "        'e_quality_ethcat4': results['e_quality_ethcat4'],\n",
    "        'e_quality_ethcat5': results['e_quality_ethcat5'],\n",
    "        'e_quality_ethcat6': results['e_quality_ethcat6'],\n",
    "        'e_quality_ethcat7': results['e_quality_ethcat7'],\n",
    "        'avg_ClassI_HLA_ethcat1': results['avg_ClassI_HLA_ethcat1'],\n",
    "        'avg_ClassI_HLA_ethcat2': results['avg_ClassI_HLA_ethcat2'],\n",
    "        'avg_ClassI_HLA_ethcat4': results['avg_ClassI_HLA_ethcat4'],\n",
    "        'avg_ClassI_HLA_ethcat5': results['avg_ClassI_HLA_ethcat5'],\n",
    "        'avg_ClassI_HLA_ethcat6': results['avg_ClassI_HLA_ethcat6'],\n",
    "        'avg_ClassI_HLA_ethcat7': results['avg_ClassI_HLA_ethcat7'],\n",
    "        'HLA_ClassI_total': results['HLA_ClassI_total'],\n",
    "\n",
    "        'avg_DR_HLA_ethcat1': results['avg_DR_HLA_ethcat1'],\n",
    "        'avg_DR_HLA_ethcat2': results['avg_DR_HLA_ethcat2'],\n",
    "        'avg_DR_HLA_ethcat4': results['avg_DR_HLA_ethcat4'],\n",
    "        'avg_DR_HLA_ethcat5': results['avg_DR_HLA_ethcat5'],\n",
    "        'avg_DR_HLA_ethcat6': results['avg_DR_HLA_ethcat6'],\n",
    "        'avg_DR_HLA_ethcat7': results['avg_DR_HLA_ethcat7'],\n",
    "        'HLA_DR_total': results['HLA_DR_total'],\n",
    "        \n",
    "\n",
    "        'avg_DQ_HLA_ethcat1': results['avg_DQ_HLA_ethcat1'],\n",
    "        'avg_DQ_HLA_ethcat2': results['avg_DQ_HLA_ethcat2'],\n",
    "        'avg_DQ_HLA_ethcat4': results['avg_DQ_HLA_ethcat4'],\n",
    "        'avg_DQ_HLA_ethcat5': results['avg_DQ_HLA_ethcat5'],\n",
    "        'avg_DQ_HLA_ethcat6': results['avg_DQ_HLA_ethcat6'],\n",
    "        'avg_DQ_HLA_ethcat7': results['avg_DQ_HLA_ethcat7'],\n",
    "        'HLA_DQ_total': results['HLA_DQ_total'],\n",
    "\n",
    "        'F_per_ethcat':   results['F_per_ethcat'],   \n",
    "        'L_per_ethcat':   results['L_per_ethcat'],   \n",
    "        'F_total': results['F_total'],\n",
    "        'lr_quality_total': results['lr_quality_total'],\n",
    "        'hr_quality_total': results['hr_quality_total'],\n",
    "        'e_quality_total': results['e_quality_total'],\n",
    "        'Waiting_time_mean': results['Waiting_time_mean']\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(simulations_results)\n",
    "\n",
    "list_HLA_ClassI_total = df_results['HLA_ClassI_total'].tolist()\n",
    "\n",
    "\n",
    "list_HLA_DR_total = df_results['HLA_DR_total'].tolist()\n",
    "\n",
    "\n",
    "list_HLA_DQ_total = df_results['HLA_DQ_total'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "lists_ClassI = {\n",
    "    1: df_results['avg_ClassI_HLA_ethcat1'].dropna().tolist(),\n",
    "    2: df_results['avg_ClassI_HLA_ethcat2'].dropna().tolist(),\n",
    "    4: df_results['avg_ClassI_HLA_ethcat4'].dropna().tolist(),\n",
    "    5: df_results['avg_ClassI_HLA_ethcat5'].dropna().tolist(),\n",
    "    6: df_results['avg_ClassI_HLA_ethcat6'].dropna().tolist(),\n",
    "    7: df_results['avg_ClassI_HLA_ethcat7'].dropna().tolist()\n",
    "}\n",
    "\n",
    "lists_DR = {\n",
    "    1: df_results['avg_DR_HLA_ethcat1'].dropna().tolist(),\n",
    "    2: df_results['avg_DR_HLA_ethcat2'].dropna().tolist(),\n",
    "    4: df_results['avg_DR_HLA_ethcat4'].dropna().tolist(),\n",
    "    5: df_results['avg_DR_HLA_ethcat5'].dropna().tolist(),\n",
    "    6: df_results['avg_DR_HLA_ethcat6'].dropna().tolist(),\n",
    "    7: df_results['avg_DR_HLA_ethcat7'].dropna().tolist()\n",
    "}\n",
    "\n",
    "lists_DQ = {\n",
    "    1: df_results['avg_DQ_HLA_ethcat1'].dropna().tolist(),\n",
    "    2: df_results['avg_DQ_HLA_ethcat2'].dropna().tolist(),\n",
    "    4: df_results['avg_DQ_HLA_ethcat4'].dropna().tolist(),\n",
    "    5: df_results['avg_DQ_HLA_ethcat5'].dropna().tolist(),\n",
    "    6: df_results['avg_DQ_HLA_ethcat6'].dropna().tolist(),\n",
    "    7: df_results['avg_DQ_HLA_ethcat7'].dropna().tolist()\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ethnicity(s)</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Transplants</th>\n",
       "      <th>F(s) (Matched)</th>\n",
       "      <th>HLA(s) Antigen</th>\n",
       "      <th>HLA(s) Allele</th>\n",
       "      <th>HLA(s) Eplets</th>\n",
       "      <th>Waiting Time</th>\n",
       "      <th>L(s) (Left Unmatched)</th>\n",
       "      <th>1-F(s)-L(s) (Still in KEP)</th>\n",
       "      <th>HLA ClassI</th>\n",
       "      <th>HLA DR</th>\n",
       "      <th>HLA DQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>592.24</td>\n",
       "      <td>374.92</td>\n",
       "      <td>0.633 [0.630; 0.636]</td>\n",
       "      <td>2.447 [2.438; 2.456]</td>\n",
       "      <td>1.663 [1.653; 1.673]</td>\n",
       "      <td>53.434 [53.356; 53.512]</td>\n",
       "      <td>4.183000</td>\n",
       "      <td>0.295 [0.291; 0.298]</td>\n",
       "      <td>0.072</td>\n",
       "      <td>2.446 [2.436; 2.457]</td>\n",
       "      <td>1.135 [1.130; 1.141]</td>\n",
       "      <td>1.312 [1.307; 1.317]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>169.69</td>\n",
       "      <td>110.76</td>\n",
       "      <td>0.653 [0.646; 0.660]</td>\n",
       "      <td>2.261 [2.245; 2.276]</td>\n",
       "      <td>1.341 [1.326; 1.356]</td>\n",
       "      <td>52.418 [52.300; 52.536]</td>\n",
       "      <td>4.026000</td>\n",
       "      <td>0.280 [0.273; 0.287]</td>\n",
       "      <td>0.067</td>\n",
       "      <td>1.939 [1.919; 1.959]</td>\n",
       "      <td>0.993 [0.984; 1.003]</td>\n",
       "      <td>1.267 [1.258; 1.277]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>147.11</td>\n",
       "      <td>97.18</td>\n",
       "      <td>0.661 [0.654; 0.667]</td>\n",
       "      <td>2.236 [2.219; 2.253]</td>\n",
       "      <td>1.334 [1.318; 1.350]</td>\n",
       "      <td>52.291 [52.173; 52.408]</td>\n",
       "      <td>4.199000</td>\n",
       "      <td>0.279 [0.271; 0.286]</td>\n",
       "      <td>0.061</td>\n",
       "      <td>2.183 [2.162; 2.204]</td>\n",
       "      <td>1.016 [1.006; 1.025]</td>\n",
       "      <td>1.221 [1.209; 1.232]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>55.43</td>\n",
       "      <td>34.87</td>\n",
       "      <td>0.629 [0.618; 0.640]</td>\n",
       "      <td>2.138 [2.112; 2.165]</td>\n",
       "      <td>1.084 [1.057; 1.111]</td>\n",
       "      <td>51.416 [51.214; 51.618]</td>\n",
       "      <td>4.493000</td>\n",
       "      <td>0.303 [0.292; 0.315]</td>\n",
       "      <td>0.067</td>\n",
       "      <td>2.039 [2.000; 2.077]</td>\n",
       "      <td>0.909 [0.892; 0.927]</td>\n",
       "      <td>1.229 [1.211; 1.247]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>7.93</td>\n",
       "      <td>5.39</td>\n",
       "      <td>0.680 [0.656; 0.704]</td>\n",
       "      <td>2.334 [2.262; 2.405]</td>\n",
       "      <td>1.453 [1.372; 1.535]</td>\n",
       "      <td>52.857 [52.357; 53.357]</td>\n",
       "      <td>2.972000</td>\n",
       "      <td>0.254 [0.226; 0.281]</td>\n",
       "      <td>0.067</td>\n",
       "      <td>2.262 [2.160; 2.364]</td>\n",
       "      <td>0.999 [0.951; 1.047]</td>\n",
       "      <td>1.335 [1.279; 1.391]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>5.90</td>\n",
       "      <td>2.72</td>\n",
       "      <td>0.461 [0.435; 0.487]</td>\n",
       "      <td>2.030 [1.914; 2.146]</td>\n",
       "      <td>1.252 [1.146; 1.359]</td>\n",
       "      <td>52.904 [52.246; 53.562]</td>\n",
       "      <td>4.838000</td>\n",
       "      <td>0.431 [0.401; 0.460]</td>\n",
       "      <td>0.108</td>\n",
       "      <td>1.738 [1.619; 1.858]</td>\n",
       "      <td>0.843 [0.782; 0.903]</td>\n",
       "      <td>1.188 [1.108; 1.267]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Entire Population</td>\n",
       "      <td>978.30</td>\n",
       "      <td>625.84</td>\n",
       "      <td>0.640 [0.637; 0.642]</td>\n",
       "      <td>2.362 [2.355; 2.368]</td>\n",
       "      <td>1.519 [1.511; 1.527]</td>\n",
       "      <td>52.957 [52.901; 53.013]</td>\n",
       "      <td>4.171801</td>\n",
       "      <td>0.291 [0.288; 0.294]</td>\n",
       "      <td>0.069</td>\n",
       "      <td>2.292 [2.284; 2.300]</td>\n",
       "      <td>1.077 [1.072; 1.081]</td>\n",
       "      <td>1.285 [1.281; 1.289]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Ethnicity(s)  Arrivals  Transplants        F(s) (Matched)  \\\n",
       "0                  1    592.24       374.92  0.633 [0.630; 0.636]   \n",
       "1                  2    169.69       110.76  0.653 [0.646; 0.660]   \n",
       "2                  4    147.11        97.18  0.661 [0.654; 0.667]   \n",
       "3                  5     55.43        34.87  0.629 [0.618; 0.640]   \n",
       "4                  6      7.93         5.39  0.680 [0.656; 0.704]   \n",
       "5                  7      5.90         2.72  0.461 [0.435; 0.487]   \n",
       "6  Entire Population    978.30       625.84  0.640 [0.637; 0.642]   \n",
       "\n",
       "         HLA(s) Antigen         HLA(s) Allele            HLA(s) Eplets  \\\n",
       "0  2.447 [2.438; 2.456]  1.663 [1.653; 1.673]  53.434 [53.356; 53.512]   \n",
       "1  2.261 [2.245; 2.276]  1.341 [1.326; 1.356]  52.418 [52.300; 52.536]   \n",
       "2  2.236 [2.219; 2.253]  1.334 [1.318; 1.350]  52.291 [52.173; 52.408]   \n",
       "3  2.138 [2.112; 2.165]  1.084 [1.057; 1.111]  51.416 [51.214; 51.618]   \n",
       "4  2.334 [2.262; 2.405]  1.453 [1.372; 1.535]  52.857 [52.357; 53.357]   \n",
       "5  2.030 [1.914; 2.146]  1.252 [1.146; 1.359]  52.904 [52.246; 53.562]   \n",
       "6  2.362 [2.355; 2.368]  1.519 [1.511; 1.527]  52.957 [52.901; 53.013]   \n",
       "\n",
       "   Waiting Time L(s) (Left Unmatched) 1-F(s)-L(s) (Still in KEP)  \\\n",
       "0      4.183000  0.295 [0.291; 0.298]                      0.072   \n",
       "1      4.026000  0.280 [0.273; 0.287]                      0.067   \n",
       "2      4.199000  0.279 [0.271; 0.286]                      0.061   \n",
       "3      4.493000  0.303 [0.292; 0.315]                      0.067   \n",
       "4      2.972000  0.254 [0.226; 0.281]                      0.067   \n",
       "5      4.838000  0.431 [0.401; 0.460]                      0.108   \n",
       "6      4.171801  0.291 [0.288; 0.294]                      0.069   \n",
       "\n",
       "             HLA ClassI                HLA DR                HLA DQ  \n",
       "0  2.446 [2.436; 2.457]  1.135 [1.130; 1.141]  1.312 [1.307; 1.317]  \n",
       "1  1.939 [1.919; 1.959]  0.993 [0.984; 1.003]  1.267 [1.258; 1.277]  \n",
       "2  2.183 [2.162; 2.204]  1.016 [1.006; 1.025]  1.221 [1.209; 1.232]  \n",
       "3  2.039 [2.000; 2.077]  0.909 [0.892; 0.927]  1.229 [1.211; 1.247]  \n",
       "4  2.262 [2.160; 2.364]  0.999 [0.951; 1.047]  1.335 [1.279; 1.391]  \n",
       "5  1.738 [1.619; 1.858]  0.843 [0.782; 0.903]  1.188 [1.108; 1.267]  \n",
       "6  2.292 [2.284; 2.300]  1.077 [1.072; 1.081]  1.285 [1.281; 1.289]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "ethnicity = [1, 2, 4, 5, 6, 7]\n",
    "n_sim = len(df_results)\n",
    "\n",
    "def media_ci(list):\n",
    "    m = np.mean(list)\n",
    "    s = np.std(list, ddof=1)\n",
    "    low, high = st.t.interval(0.95, len(list)-1, loc=m, scale=s/np.sqrt(len(list)))\n",
    "    return m, f\"{m:.3f} [{low:.3f}; {high:.3f}]\"\n",
    "\n",
    "rows = []\n",
    "# Reporting per ethnicity \n",
    "\n",
    "for e in ethnicity:\n",
    "    arrivals_e = df_results['arrivals_by_ethcat'].apply(lambda d: d.get(e, 0)).sum() / n_sim\n",
    "    transplants_e = df_results['cycles'].apply(\n",
    "        lambda cycles: sum(1 for c in cycles for n in c if recipients.loc[n, 'ETHCAT'] == e)\n",
    "    ).sum() / n_sim\n",
    "\n",
    "    F_vals = df_results['F_per_ethcat'].apply(lambda d: d[e])\n",
    "    _, txt_F = media_ci(F_vals)\n",
    "    std_F = np.std(F_vals, ddof=1)\n",
    "\n",
    "    lr_quality = df_results[f'lr_quality_ethcat{e}']\n",
    "    _, txt_antigen = media_ci(lr_quality)\n",
    "\n",
    "    hr_quality = df_results[f'hr_quality_ethcat{e}']\n",
    "    _, txt_allele = media_ci(hr_quality)\n",
    "\n",
    "    e_quality  = df_results[f'e_quality_ethcat{e}']\n",
    "    _, txt_eplet = media_ci(e_quality)\n",
    "\n",
    "    waiting = df_results[f'waiting_time_mean{e}'].mean()\n",
    "\n",
    "    L_vals = df_results['L_per_ethcat'].apply(lambda d: d[e])\n",
    "    _, txt_L = media_ci(L_vals)\n",
    "\n",
    "    f_mean = np.mean(F_vals)\n",
    "    l_mean = np.mean(L_vals)\n",
    "    still_in_kep = round(1 - f_mean - l_mean, 3)\n",
    "\n",
    "\n",
    "    list_classi = lists_ClassI[e]\n",
    "\n",
    "    _, txt_classi = media_ci(list_classi)\n",
    "\n",
    "    list_dr = lists_DR[e]\n",
    "    _, txt_dr = media_ci(list_dr)\n",
    "\n",
    "    list_dq = lists_DQ[e]\n",
    "    _, txt_dq = media_ci(list_dq)\n",
    "\n",
    "\n",
    "    rows.append({\n",
    "        'Ethnicity(s)': e,\n",
    "        'Arrivals': round(arrivals_e, 2),\n",
    "        'Transplants': round(transplants_e, 2),\n",
    "        'F(s) (Matched)': txt_F,\n",
    "        'HLA(s) Antigen': txt_antigen,\n",
    "        'HLA(s) Allele': txt_allele,\n",
    "        'HLA(s) Eplets': txt_eplet,\n",
    "        'Waiting Time': round(waiting, 3),\n",
    "        'L(s) (Left Unmatched)': txt_L,\n",
    "        '1-F(s)-L(s) (Still in KEP)': still_in_kep,\n",
    "        'HLA ClassI': txt_classi,\n",
    "        'HLA DR': txt_dr,\n",
    "        'HLA DQ': txt_dq,\n",
    "\n",
    "    })\n",
    "\n",
    "# Reporting the totals\n",
    "\n",
    "total_arrivals = sum(df_results['arrivals_by_ethcat'].apply(lambda d: sum(d.values()))) / n_sim\n",
    "total_transplants = df_results['cycles'].apply(lambda cycles: sum(len(c) for c in cycles)).sum() / n_sim\n",
    "F_total = total_transplants / total_arrivals\n",
    "\n",
    "\n",
    "F_s_total, txt_F_total = media_ci(df_results['F_total'])\n",
    "\n",
    "L_vals_flat = (df_results['total_departures'] / df_results['total_entries']).tolist()\n",
    "L_s_total, ic_L_total = media_ci(L_vals_flat)\n",
    "txt_L_total = ic_L_total\n",
    "\n",
    "_, txt_total_lr_quality = media_ci(df_results['lr_quality_total'])\n",
    "_,txt_total_hr_quality = media_ci(df_results['hr_quality_total'])\n",
    "_,txt_total_e_quality= media_ci(df_results['e_quality_total'])\n",
    "\n",
    "txt_waiting_time_mean = np.mean(df_results['Waiting_time_mean'])\n",
    "\n",
    "F_vals_flat = [df_results['F_per_ethcat'].iloc[i][e] for i in range(n_sim) for e in ethnicity]\n",
    "std_F_total = np.std(F_vals_flat, ddof=1)\n",
    "\n",
    "f_total_mean = F_total  \n",
    "total_outgoing = sum(\n",
    "    df_results['L_per_ethcat'].apply(lambda d: sum(d.values()))\n",
    ") / n_sim\n",
    "\n",
    "l_total_mean = total_outgoing / total_arrivals\n",
    "still_in_kep_total = round(1 - F_total - l_total_mean, 3)\n",
    "\n",
    "\n",
    "_, txt_classi_total = media_ci(list_HLA_ClassI_total)\n",
    "\n",
    "_, txt_dr_total = media_ci(list_HLA_DR_total )\n",
    "\n",
    "_, txt_dq_total = media_ci(list_HLA_DQ_total )\n",
    "\n",
    "rows.append({\n",
    "    'Ethnicity(s)': 'Entire Population',\n",
    "    'Arrivals': round(total_arrivals, 2),\n",
    "    'Transplants': round(total_transplants, 2),\n",
    "    'F(s) (Matched)': txt_F_total,\n",
    "    'HLA(s) Antigen': txt_total_lr_quality,\n",
    "    'HLA(s) Allele': txt_total_hr_quality,\n",
    "    'HLA(s) Eplets': txt_total_e_quality,\n",
    "    'Waiting Time': txt_waiting_time_mean,\n",
    "    'L(s) (Left Unmatched)': txt_L_total,\n",
    "    '1-F(s)-L(s) (Still in KEP)': f\"{(1 - L_s_total - F_s_total):.3f}\",\n",
    "    'HLA ClassI': txt_classi_total,\n",
    "    'HLA DR': txt_dr_total,\n",
    "    'HLA DQ': txt_dq_total\n",
    "})\n",
    "\n",
    "df_final_table = pd.DataFrame(rows)\n",
    "display(df_final_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ethnicity(s)</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Transplants</th>\n",
       "      <th>F(s) (Matched)</th>\n",
       "      <th>HLA(s) Antigen</th>\n",
       "      <th>HLA(s) Allele</th>\n",
       "      <th>HLA(s) Eplets</th>\n",
       "      <th>Waiting Time</th>\n",
       "      <th>L(s) (Left Unmatched)</th>\n",
       "      <th>1-F(s)-L(s) (Still in KEP)</th>\n",
       "      <th>HLA ClassI</th>\n",
       "      <th>HLA DR</th>\n",
       "      <th>HLA DQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>591.56</td>\n",
       "      <td>398.28</td>\n",
       "      <td>0.673 [0.669; 0.677]</td>\n",
       "      <td>4.683 [4.668; 4.697]</td>\n",
       "      <td>3.394 [3.379; 3.410]</td>\n",
       "      <td>93.036 [92.907; 93.165]</td>\n",
       "      <td>3.0950</td>\n",
       "      <td>0.290 [0.287; 0.294]</td>\n",
       "      <td>0.036</td>\n",
       "      <td>2.559 [2.548; 2.570]</td>\n",
       "      <td>0.940 [0.935; 0.946]</td>\n",
       "      <td>1.183 [1.178; 1.189]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>170.05</td>\n",
       "      <td>115.52</td>\n",
       "      <td>0.679 [0.672; 0.686]</td>\n",
       "      <td>4.174 [4.148; 4.199]</td>\n",
       "      <td>2.534 [2.509; 2.558]</td>\n",
       "      <td>90.787 [90.567; 91.006]</td>\n",
       "      <td>3.1280</td>\n",
       "      <td>0.286 [0.279; 0.293]</td>\n",
       "      <td>0.035</td>\n",
       "      <td>2.138 [2.117; 2.159]</td>\n",
       "      <td>0.851 [0.839; 0.863]</td>\n",
       "      <td>1.184 [1.174; 1.195]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>147.22</td>\n",
       "      <td>97.92</td>\n",
       "      <td>0.665 [0.658; 0.672]</td>\n",
       "      <td>4.324 [4.298; 4.350]</td>\n",
       "      <td>2.690 [2.663; 2.717]</td>\n",
       "      <td>91.165 [90.896; 91.433]</td>\n",
       "      <td>3.2640</td>\n",
       "      <td>0.299 [0.291; 0.307]</td>\n",
       "      <td>0.036</td>\n",
       "      <td>2.327 [2.305; 2.349]</td>\n",
       "      <td>0.875 [0.863; 0.888]</td>\n",
       "      <td>1.122 [1.110; 1.133]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>55.31</td>\n",
       "      <td>36.93</td>\n",
       "      <td>0.667 [0.654; 0.680]</td>\n",
       "      <td>3.935 [3.900; 3.969]</td>\n",
       "      <td>2.033 [2.001; 2.065]</td>\n",
       "      <td>84.352 [83.947; 84.757]</td>\n",
       "      <td>3.3860</td>\n",
       "      <td>0.299 [0.286; 0.312]</td>\n",
       "      <td>0.033</td>\n",
       "      <td>2.116 [2.084; 2.147]</td>\n",
       "      <td>0.733 [0.714; 0.752]</td>\n",
       "      <td>1.086 [1.067; 1.105]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>7.94</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.740 [0.714; 0.767]</td>\n",
       "      <td>4.219 [4.121; 4.317]</td>\n",
       "      <td>2.873 [2.765; 2.981]</td>\n",
       "      <td>83.872 [82.857; 84.887]</td>\n",
       "      <td>2.1020</td>\n",
       "      <td>0.234 [0.207; 0.261]</td>\n",
       "      <td>0.025</td>\n",
       "      <td>2.160 [2.077; 2.242]</td>\n",
       "      <td>0.856 [0.809; 0.903]</td>\n",
       "      <td>1.203 [1.150; 1.256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>5.93</td>\n",
       "      <td>2.67</td>\n",
       "      <td>0.450 [0.415; 0.486]</td>\n",
       "      <td>4.142 [3.998; 4.286]</td>\n",
       "      <td>2.322 [2.163; 2.481]</td>\n",
       "      <td>93.955 [92.581; 95.329]</td>\n",
       "      <td>5.6300</td>\n",
       "      <td>0.474 [0.437; 0.512]</td>\n",
       "      <td>0.075</td>\n",
       "      <td>2.081 [1.965; 2.197]</td>\n",
       "      <td>0.793 [0.717; 0.869]</td>\n",
       "      <td>1.268 [1.188; 1.349]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Entire Population</td>\n",
       "      <td>978.01</td>\n",
       "      <td>657.20</td>\n",
       "      <td>0.672 [0.669; 0.675]</td>\n",
       "      <td>4.491 [4.481; 4.502]</td>\n",
       "      <td>3.053 [3.041; 3.065]</td>\n",
       "      <td>91.796 [91.698; 91.894]</td>\n",
       "      <td>3.1462</td>\n",
       "      <td>0.292 [0.289; 0.296]</td>\n",
       "      <td>0.036</td>\n",
       "      <td>2.420 [2.412; 2.429]</td>\n",
       "      <td>0.902 [0.897; 0.907]</td>\n",
       "      <td>1.169 [1.165; 1.173]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Ethnicity(s)  Arrivals  Transplants        F(s) (Matched)  \\\n",
       "0                  1    591.56       398.28  0.673 [0.669; 0.677]   \n",
       "1                  2    170.05       115.52  0.679 [0.672; 0.686]   \n",
       "2                  4    147.22        97.92  0.665 [0.658; 0.672]   \n",
       "3                  5     55.31        36.93  0.667 [0.654; 0.680]   \n",
       "4                  6      7.94         5.88  0.740 [0.714; 0.767]   \n",
       "5                  7      5.93         2.67  0.450 [0.415; 0.486]   \n",
       "6  Entire Population    978.01       657.20  0.672 [0.669; 0.675]   \n",
       "\n",
       "         HLA(s) Antigen         HLA(s) Allele            HLA(s) Eplets  \\\n",
       "0  4.683 [4.668; 4.697]  3.394 [3.379; 3.410]  93.036 [92.907; 93.165]   \n",
       "1  4.174 [4.148; 4.199]  2.534 [2.509; 2.558]  90.787 [90.567; 91.006]   \n",
       "2  4.324 [4.298; 4.350]  2.690 [2.663; 2.717]  91.165 [90.896; 91.433]   \n",
       "3  3.935 [3.900; 3.969]  2.033 [2.001; 2.065]  84.352 [83.947; 84.757]   \n",
       "4  4.219 [4.121; 4.317]  2.873 [2.765; 2.981]  83.872 [82.857; 84.887]   \n",
       "5  4.142 [3.998; 4.286]  2.322 [2.163; 2.481]  93.955 [92.581; 95.329]   \n",
       "6  4.491 [4.481; 4.502]  3.053 [3.041; 3.065]  91.796 [91.698; 91.894]   \n",
       "\n",
       "   Waiting Time L(s) (Left Unmatched) 1-F(s)-L(s) (Still in KEP)  \\\n",
       "0        3.0950  0.290 [0.287; 0.294]                      0.036   \n",
       "1        3.1280  0.286 [0.279; 0.293]                      0.035   \n",
       "2        3.2640  0.299 [0.291; 0.307]                      0.036   \n",
       "3        3.3860  0.299 [0.286; 0.312]                      0.033   \n",
       "4        2.1020  0.234 [0.207; 0.261]                      0.025   \n",
       "5        5.6300  0.474 [0.437; 0.512]                      0.075   \n",
       "6        3.1462  0.292 [0.289; 0.296]                      0.036   \n",
       "\n",
       "             HLA ClassI                HLA DR                HLA DQ  \n",
       "0  2.559 [2.548; 2.570]  0.940 [0.935; 0.946]  1.183 [1.178; 1.189]  \n",
       "1  2.138 [2.117; 2.159]  0.851 [0.839; 0.863]  1.184 [1.174; 1.195]  \n",
       "2  2.327 [2.305; 2.349]  0.875 [0.863; 0.888]  1.122 [1.110; 1.133]  \n",
       "3  2.116 [2.084; 2.147]  0.733 [0.714; 0.752]  1.086 [1.067; 1.105]  \n",
       "4  2.160 [2.077; 2.242]  0.856 [0.809; 0.903]  1.203 [1.150; 1.256]  \n",
       "5  2.081 [1.965; 2.197]  0.793 [0.717; 0.869]  1.268 [1.188; 1.349]  \n",
       "6  2.420 [2.412; 2.429]  0.902 [0.897; 0.907]  1.169 [1.165; 1.173]  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_table # este tiene la dision con P "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPARACION DE TABLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comparative metrics (mean across simulations) ===\n",
      "           metric       base   modified     delta\n",
      "  selected_cycles 257.920000 257.150000 -0.770000\n",
      "    selected_arcs 656.420000 655.740000 -0.680000\n",
      "          F_total   0.671341   0.670171 -0.001171\n",
      "Waiting_time_mean   3.156223   3.149267 -0.006956\n",
      " lr_quality_total   4.516773   4.486849 -0.029923\n",
      " hr_quality_total   3.085509   3.053586 -0.031922\n",
      "  e_quality_total  91.923726  91.734340 -0.189386\n",
      "\n",
      "=== Selected arcs by ethnicity (mean per simulation) ===\n",
      " ethcat       base   modified      delta\n",
      "      1 414.130000 399.290000 -14.840000\n",
      "      2 107.960000 112.270000   4.310000\n",
      "      4  91.650000  98.230000   6.580000\n",
      "      5  34.040000  36.940000   2.900000\n",
      "      6   6.040000   6.110000   0.070000\n",
      "      7   2.600000   2.900000   0.300000\n",
      "\n",
      "=== Common cycle (sim=0) ===\n",
      "Cycle: (735, 756)\n",
      "Approx. total (base):    2.000808\n",
      "Approx. total (modified):1.996806\n",
      "Delta total:             -0.004002\n",
      "\n",
      "Per-edge breakdown (u->v):\n",
      "  u   v  ETHCAT   m_base     w_hr  contrib_base    m_mod  contrib_mod  delta_contrib\n",
      "735 756       1 1.000000 3.000000      1.000303 0.998000     0.998302      -0.002001\n",
      "756 735       1 1.000000 5.000000      1.000505 0.998000     0.998504      -0.002001\n",
      "\n",
      "Cycles ONLY in base (examples): [(8, 36, 953), (54, 281, 264), (98, 339), (33, 158, 287), (886, 952)]\n",
      "Cycles ONLY in modified (examples): [(683, 718), (120, 373, 447), (249, 292, 506), (634, 804, 897), (827, 903)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# UTILITIES\n",
    "\n",
    "def _canonical_cycle(cycle):\n",
    "    #Return a representation of a cycle to enable set comparison.\n",
    "    c = list(cycle)\n",
    "    i0 = min(range(len(c)), key=lambda i: c[i])\n",
    "    r = c[i0:] + c[:i0]\n",
    "    rev = list(reversed(r))\n",
    "    return tuple(r) if r <= rev else tuple(rev)\n",
    "\n",
    "def _edge_weight_hr(u, v):\n",
    "    #Change the resolution here\n",
    "    return float(hla_lr.iloc[v, u])\n",
    "\n",
    "def _eth_of(v):\n",
    "    try:\n",
    "        return int(recipients.loc[v, 'ETHCAT'])\n",
    "    except Exception:\n",
    "        return int(recipients.loc[recipients['Nodo'] == v, 'ETHCAT'].iloc[0])\n",
    "\n",
    "def _edge_contrib_approx(u, v, multipliers, P_est, Z=10.0):\n",
    "   \n",
    "    #Approximate contribution of edge u->v to the objective:\n",
    "    #  contrib ≈ m_eth * (1 + w_hr / (P_est * Z))\n",
    "    eth = _eth_of(v)\n",
    "    m = multipliers.get(eth, 1.0)\n",
    "    w = _edge_weight_hr(u, v)\n",
    "    contrib = m * (1.0 + w / (P_est * Z))\n",
    "    return contrib, eth, m, w\n",
    "\n",
    "def _cycle_contrib_table(cycle, multipliers, P_est, Z=10.0):\n",
    "    rows, total = [], 0.0\n",
    "    nodes = list(cycle)\n",
    "    for u, v in zip(nodes, nodes[1:] + nodes[:1]):\n",
    "        contrib, eth, m, w = _edge_contrib_approx(u, v, multipliers, P_est, Z)\n",
    "        rows.append({'u': u, 'v': v, 'ETHCAT': eth, 'm': m, 'w_hr': w, 'contrib': contrib})\n",
    "        total += contrib\n",
    "    return total, pd.DataFrame(rows)\n",
    "\n",
    "def _count_arcs_by_eth(cycles, eths=(1,2,4,5,6,7)):\n",
    "    #Count edges u->v by ethnicity\n",
    "    counts = {e: 0 for e in eths}\n",
    "    for cyc in cycles:\n",
    "        nodes = list(cyc)\n",
    "        for u, v in zip(nodes, nodes[1:] + nodes[:1]):\n",
    "            eth = _eth_of(v)\n",
    "            if eth in counts:\n",
    "                counts[eth] += 1\n",
    "    return counts\n",
    "\n",
    "def _summarize_one_result(res_dict):\n",
    "    cycles = res_dict['historial_cycles']\n",
    "    selected_cycles = len(cycles)\n",
    "    selected_arcs   = sum(len(c) for c in cycles)\n",
    "    arcs_by_eth = _count_arcs_by_eth(cycles)\n",
    "\n",
    "    out = {\n",
    "        'selected_cycles': selected_cycles,\n",
    "        'selected_arcs': selected_arcs,\n",
    "        'F_total': res_dict.get('F_total', np.nan),\n",
    "        'Waiting_time_mean': res_dict.get('Waiting_time_mean', np.nan),\n",
    "        'lr_quality_total': res_dict.get('lr_quality_total', np.nan),\n",
    "        'hr_quality_total': res_dict.get('hr_quality_total', np.nan),\n",
    "        'e_quality_total':  res_dict.get('e_quality_total',  np.nan),\n",
    "    }\n",
    "    for e, c in arcs_by_eth.items():\n",
    "        out[f'arcs_eth{e}'] = c\n",
    "    return out\n",
    "\n",
    "# RUN MANY SIMULATIONS WITH CONTROLLED SEEDS\n",
    "\n",
    "\n",
    "def run_many(multipliers, n_sims=100, base_seed=12345):\n",
    "    \n",
    "    results, rows = [], []\n",
    "    for i in range(n_sims):\n",
    "        np.random.seed(base_seed + i)  # same randomness per index\n",
    "        res = run_simulation(\n",
    "            total_time=10*12,\n",
    "            arrival_rate=(990/(10*12)),\n",
    "            departure_rate=((990/(10*12))*0.29),\n",
    "            match_run=3,\n",
    "            pairs=pairs,\n",
    "            compatibility=compatibility,\n",
    "            hla_lr=hla_lr,\n",
    "            hla_hr=hla_hr,\n",
    "            hla_eplet=hla_eplet,\n",
    "            multipliers_ethnicity=multipliers\n",
    "        )\n",
    "        results.append(res)\n",
    "        row = {'sim': i}\n",
    "        row.update(_summarize_one_result(res))\n",
    "        rows.append(row)\n",
    "    summary_df = pd.DataFrame(rows)\n",
    "    return results, summary_df\n",
    "\n",
    "# BASE vs MODIFIED COMPARISON\n",
    "\n",
    "def compare_scenarios_via_sim(\n",
    "    multipliers_base=None,\n",
    "    multipliers_mod=None,\n",
    "    n_sims=100,\n",
    "    seed=12345,\n",
    "    Z=10.0\n",
    "):\n",
    "\n",
    "    if multipliers_base is None:\n",
    "        multipliers_base = {1:1.0, 2:1.0, 4:1.0, 5:1.0, 6:1.0, 7:1.0}\n",
    "    if multipliers_mod is None:\n",
    "        multipliers_mod  = {1:0.998, 2:1.0, 4:1.0, 5:1.002, 6:1.0, 7:1.0}\n",
    "\n",
    "    base_results, base_df = run_many(multipliers_base, n_sims=n_sims, base_seed=seed)\n",
    "    mod_results,  mod_df  = run_many(multipliers_mod,  n_sims=n_sims, base_seed=seed)\n",
    "\n",
    "    # 1) means across simulations\n",
    "    metrics = ['selected_cycles', 'selected_arcs', 'F_total', 'Waiting_time_mean',\n",
    "               'lr_quality_total', 'hr_quality_total', 'e_quality_total']\n",
    "    comp_rows = []\n",
    "    for m in metrics:\n",
    "        b_mean = float(base_df[m].mean())\n",
    "        d_mean = float(mod_df[m].mean())\n",
    "        comp_rows.append({'metric': m, 'base': b_mean, 'modified': d_mean, 'delta': d_mean - b_mean})\n",
    "    metrics_comp = pd.DataFrame(comp_rows)\n",
    "\n",
    "    # selected arcs by ethnicity \n",
    "    eth_list = [1,2,4,5,6,7]\n",
    "    arcs_rows = []\n",
    "    for e in eth_list:\n",
    "        col = f'arcs_eth{e}'\n",
    "        b_mean = float(base_df[col].mean()) if col in base_df.columns else float('nan')\n",
    "        d_mean = float(mod_df[col].mean())  if col in mod_df.columns  else float('nan')\n",
    "        arcs_rows.append({'ethcat': e, 'base': b_mean, 'modified': d_mean, 'delta': d_mean - b_mean})\n",
    "    arcs_by_eth_comp = pd.DataFrame(arcs_rows)\n",
    "\n",
    "    common_info = None\n",
    "    only_base, only_mod = None, None\n",
    "    common_cycle_table = None\n",
    "    P_est = len(pairs.index)\n",
    "\n",
    "    for i in range(n_sims):\n",
    "        cyc_b = base_results[i]['historial_cycles']\n",
    "        cyc_m = mod_results[i]['historial_cycles']\n",
    "        set_b = { _canonical_cycle(c) for c in cyc_b }\n",
    "        set_m = { _canonical_cycle(c) for c in cyc_m }\n",
    "        inter = list(set_b & set_m)\n",
    "        if inter:\n",
    "            cyc = list(inter[0])\n",
    "            total_b, tbl_b = _cycle_contrib_table(cyc, multipliers_base, P_est, Z=Z)\n",
    "            total_m, tbl_m = _cycle_contrib_table(cyc, multipliers_mod,  P_est, Z=Z)\n",
    "            common_cycle_table = tbl_b.copy()\n",
    "            common_cycle_table.rename(columns={'m':'m_base','contrib':'contrib_base'}, inplace=True)\n",
    "            common_cycle_table['m_mod']         = tbl_m['m'].values\n",
    "            common_cycle_table['contrib_mod']   = tbl_m['contrib'].values\n",
    "            common_cycle_table['delta_contrib'] = common_cycle_table['contrib_mod'] - common_cycle_table['contrib_base']\n",
    "\n",
    "            common_info = {\n",
    "                'sim_index': i,\n",
    "                'cycle': tuple(cyc),\n",
    "                'total_base': float(total_b),\n",
    "                'total_mod': float(total_m),\n",
    "                'delta_total': float(total_m - total_b),\n",
    "            }\n",
    "            only_base = list(set_b - set_m)\n",
    "            only_mod  = list(set_m - set_b)\n",
    "            break\n",
    "\n",
    "    return metrics_comp, arcs_by_eth_comp, common_info, common_cycle_table, only_base, only_mod\n",
    "\n",
    "# EXAMPLE RUN\n",
    "\n",
    "m_base = {1:1.0,   2:1.0, 4:1.0, 5:1.0,   6:1.0, 7:1.0}\n",
    "m_mod  = {1:0.998, 2:1.0, 4:1.001, 5:1.001, 6:1.0, 7:1.0}\n",
    "\n",
    "metrics_df, arcs_by_eth_df, common_info, common_cycle_df, only_base, only_mod = compare_scenarios_via_sim(\n",
    "    multipliers_base=m_base,\n",
    "    multipliers_mod=m_mod,\n",
    "    n_sims=100,\n",
    "    seed=12345,\n",
    "    Z=10.0\n",
    ")\n",
    "\n",
    "print(\"\\n=== Comparative metrics (mean across simulations) ===\")\n",
    "print(metrics_df.to_string(index=False, float_format=lambda x: f\"{x:.6f}\" if isinstance(x,float) else str(x)))\n",
    "\n",
    "print(\"\\n=== Selected arcs by ethnicity (mean per simulation) ===\")\n",
    "print(arcs_by_eth_df.to_string(index=False, float_format=lambda x: f\"{x:.6f}\" if isinstance(x,float) else str(x)))\n",
    "\n",
    "if common_info is not None:\n",
    "    print(f\"\\n=== Common cycle (sim={common_info['sim_index']}) ===\")\n",
    "    print(f\"Cycle: {common_info['cycle']}\")\n",
    "    print(f\"Approx. total (base):    {common_info['total_base']:.6f}\")\n",
    "    print(f\"Approx. total (modified):{common_info['total_mod']:.6f}\")\n",
    "    print(f\"Delta total:             {common_info['delta_total']:+.6f}\")\n",
    "    print(\"\\nPer-edge breakdown (u->v):\")\n",
    "    print(common_cycle_df.to_string(index=False, float_format=lambda x: f'{x:.6f}' if isinstance(x,float) else str(x)))\n",
    "else:\n",
    "    print(\"\\n(No common cycle found in the inspected simulations.)\")\n",
    "\n",
    "if only_base:\n",
    "    print(f\"\\nCycles ONLY in base (examples): {only_base[:5]}\")\n",
    "if only_mod:\n",
    "    print(f\"Cycles ONLY in modified (examples): {only_mod[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Comparación entre escenario original y escenario modificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR LOW RESOLUTION ESCENARIO\n",
    "#m_base = {1:1.0,   2:1.0, 4:1.0, 5:1.0,   6:1.0, 7:1.0}\n",
    "#m_mod  = {1:0.998, 2:1.0, 4:1.001, 5:1.001, 6:1.0, 7:1.0}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR HIGH RESOLUTION ESCENARIO\n",
    "#m_base = {1:1.0,   2:1.0, 4:1.0, 5:1.0,   6:1.0, 7:1.0}\n",
    "#m_mod  = {1:0.998, 2:1.0, 4:1.0, 5:1.002, 6:1.0, 7:1.0}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "=== Selected arcs by ethnicity (mean per simulation) ===\n",
    " ethcat       base   modified      delta\n",
    "      1 411.380000 400.540000 -10.840000\n",
    "      2 105.420000 112.020000   6.600000\n",
    "      4  88.730000  94.500000   5.770000\n",
    "      5  30.140000  34.430000   4.290000\n",
    "      6   6.450000   6.670000   0.220000\n",
    "      7   2.060000   2.110000   0.050000\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR EPLET ESCENARIO\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
